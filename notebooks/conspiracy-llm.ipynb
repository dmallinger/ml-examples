{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1e3a48-97db-40fd-be8f-82a700ccc637",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf6b52-f733-48c5-ace9-acf3e510644f",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8324ac-1edc-47da-886f-c699f20ed6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "RUN_TEST = False  # used to speed up testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2471f-8a0a-41c3-885a-ce3249b60cc3",
   "metadata": {},
   "source": [
    "## Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b395c6-030e-4ea3-b282-3065e755fa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab5a5b-58e1-4e07-8072-e6fb21cf67bf",
   "metadata": {},
   "source": [
    "## Ollama Fix\n",
    "\n",
    "We need a working version of llama.cpp in our current directory. See this [github issue](https://github.com/unslothai/unsloth/issues/1495)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa461b5-a5a8-4882-9bfb-76a7cf14aac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /home/mallinger/venv/ft/lib/python3.12/site-packages (4.0.0)\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring done (0.2s)\n",
      "-- Generating done (0.1s)\n",
      "-- Build files have been written to: /home/mallinger/Code/ml-examples/notebooks/llama.cpp/build\n",
      "[  4%] Built target ggml-base\n",
      "[ 10%] Built target ggml-cpu\n",
      "[ 11%] Built target ggml\n",
      "[ 21%] Built target llama\n",
      "[ 21%] Built target build_info\n",
      "[ 26%] Built target common\n",
      "[ 27%] Built target test-tokenizer-0\n",
      "[ 28%] Built target test-sampling\n",
      "[ 30%] Built target test-grammar-parser\n",
      "[ 31%] Built target test-grammar-integration\n",
      "[ 32%] Built target test-llama-grammar\n",
      "[ 33%] Built target test-chat\n",
      "[ 34%] Built target test-json-schema-to-grammar\n",
      "[ 35%] Built target test-quantize-stats\n",
      "[ 36%] Built target test-gbnf-validator\n",
      "[ 37%] Built target test-tokenizer-1-bpe\n",
      "[ 38%] Built target test-tokenizer-1-spm\n",
      "[ 40%] Built target test-log\n",
      "[ 42%] Built target test-chat-template\n",
      "[ 43%] Built target test-arg-parser\n",
      "[ 44%] Built target test-gguf\n",
      "[ 46%] Built target test-backend-ops\n",
      "[ 47%] Built target test-model-load-cancel\n",
      "[ 48%] Built target test-autorelease\n",
      "[ 49%] Built target test-barrier\n",
      "[ 51%] Built target test-quantize-fns\n",
      "[ 52%] Built target test-quantize-perf\n",
      "[ 53%] Built target test-rope\n",
      "[ 54%] Built target mtmd\n",
      "[ 55%] Built target test-mtmd-c-api\n",
      "[ 56%] Built target test-c\n",
      "[ 57%] Built target llama-batched\n",
      "[ 58%] Built target llama-embedding\n",
      "[ 59%] Built target llama-eval-callback\n",
      "[ 60%] Built target sha256\n",
      "[ 61%] Built target xxhash\n",
      "[ 61%] Built target sha1\n",
      "[ 62%] Built target llama-gguf-hash\n",
      "[ 62%] Built target llama-gguf\n",
      "[ 63%] Built target llama-gritlm\n",
      "[ 64%] Built target llama-infill\n",
      "[ 65%] Built target llama-lookahead\n",
      "[ 65%] Built target llama-lookup\n",
      "[ 66%] Built target llama-lookup-create\n",
      "[ 67%] Built target llama-lookup-merge\n",
      "[ 68%] Built target llama-lookup-stats\n",
      "[ 69%] Built target llama-parallel\n",
      "[ 70%] Built target llama-passkey\n",
      "[ 71%] Built target llama-retrieval\n",
      "[ 72%] Built target llama-save-load-state\n",
      "[ 73%] Built target llama-simple\n",
      "[ 74%] Built target llama-simple-chat\n",
      "[ 75%] Built target llama-speculative\n",
      "[ 76%] Built target llama-speculative-simple\n",
      "[ 77%] Built target llama-gen-docs\n",
      "[ 78%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 79%] Built target llama-vdot\n",
      "[ 79%] Built target llama-q8dot\n",
      "[ 80%] Built target llama-batched-bench\n",
      "[ 81%] Built target llama-gguf-split\n",
      "[ 82%] Built target llama-imatrix\n",
      "[ 82%] Built target llama-bench\n",
      "[ 83%] Built target llama-cli\n",
      "[ 84%] Built target llama-perplexity\n",
      "[ 85%] Built target llama-quantize\n",
      "[ 87%] Built target llama-server\n",
      "[ 88%] Built target llama-run\n",
      "[ 89%] Built target llama-tokenize\n",
      "[ 89%] Built target llama-tts\n",
      "[ 90%] Built target llava\n",
      "[ 90%] Built target llava_static\n",
      "[ 91%] Built target llava_shared\n",
      "[ 92%] Built target mtmd_static\n",
      "[ 92%] Built target mtmd_shared\n",
      "[ 93%] Built target llama-llava-cli\n",
      "[ 94%] Built target llama-gemma3-cli\n",
      "[ 95%] Built target llama-minicpmv-cli\n",
      "[ 96%] Built target llama-qwen2vl-cli\n",
      "[ 97%] Built target llama-mtmd-cli\n",
      "[ 98%] Built target llama-llava-clip-quantize-cli\n",
      "[ 99%] Built target llama-cvector-generator\n",
      "[100%] Built target llama-export-lora\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake\n",
    "!if [ ! -e \"llama.cpp\" ]; then git clone https://github.com/ggml-org/llama.cpp; fi\n",
    "!cmake -B build -DLLAMA_CURL=OFF -Sllama.cpp -Bllama.cpp/build\n",
    "!cmake --build llama.cpp/build --config Release\n",
    "!cp llama.cpp/build/bin/llama-* llama.cpp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c7ae3-2260-4f0a-a9a1-590abcc7ddab",
   "metadata": {},
   "source": [
    "## List of Conspiracy Theories\n",
    "\n",
    "List of lists: One list per theory containing an enumeration of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88673dc5-da0e-453f-b751-e5a2f3ad4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSPIRACY_TOPICS = [\n",
    "    (\"JFK\", \"Lee Harvey Oswald\", \"Jack Ruby\", \"Zapruder Film\"),\n",
    "    (\"Globe\", \"Flat Earth\", \"Alderson Disk\", \"Hollow Earth\"),\n",
    "    (\"Black Helicopters\",),\n",
    "    (\"Chemtrails\", \"Contrails\"),\n",
    "    (\"Malaysia Airlines Flight MH370\",),\n",
    "    (\"New World Order\", \"NWO\", \"Globalism\", \"Free Masons\", \"Illuminati\",\n",
    "     \"The Elders of Zion\", \"Brave New World\", \"George Soros\"),\n",
    "    (\"False Flag\", \"Pearl Harbor\", \"The Oklahoma City bombing\",\n",
    "     \"2004 Madrid train bombings\", \"The 1964 Gulf of Tonkin incident\",\n",
    "     \"The Euromaidan massacre\", \"Sandy Hook\", \"Adam Lanza\", \n",
    "     \"Crisis Actors\"),\n",
    "    (\"Spygate\", \"Stefan Halper\", \"Nunes memo\",\n",
    "     \"Russian interference in the 2016 United States elections\"),\n",
    "    (\"Clinton Body Count\", \"Linda Thompson\", \"Clinton Chronicles\",\n",
    "     \"Vince Foster\", \"Don Henry and Kevin Ives\", \"Jerry Parks\",\n",
    "     \"Edward Eugene Willey\", \"Jeffrey Epstein\"),\n",
    "    (\"Barack Obama birth Certificate\", \"Birtherism\", \"Sarah Obama\",\n",
    "     \"Barack Obama's Kenyan Citizenship\"),\n",
    "    (\"Cultural Marxism\", \"Frankfurt School\", \"Cultural Bolshevism\",\n",
    "     \"LaRouche Movement\"),\n",
    "    (\"QAnon\", \"Pizzagate\", \"Cannibalistic Child Molesters in Government\",\n",
    "     \"Q\"),\n",
    "    (\"Deep State\", \"Illiberal Democracy\", \"Shadow Government\"),\n",
    "    (\"Climate Change\", \"Global Warming\", \"Global Cooling\", \"CO2\"),\n",
    "    (\"Reptilians\", \"David Icke\", \"Lizard People\"),\n",
    "    (\"Vaccines\", \"Autism\", \"mRNA Vaccines\", \"Homeopathy\",\n",
    "     \"Nosodes\", \"Vaccine Additives\", \"Polio\"),\n",
    "    (\"Covid-19\", \"Covid Origins\", \"Wuhan\", \"Gain-of-Function Research\",\n",
    "     \"Bat Coronaviruses\", \"5G\", \"Deaths from Covid\", \"Casedemic\",\n",
    "     \"Ivermectin\", \"Chloroquine\", \"Miracle Mineral Supplement\"),\n",
    "    (\"9/11\", \"9/11 Commission\", \"Twin Towers\", \"7 World Trade Center\",\n",
    "     \"U.S. invasion of Afghanistan in 2001\", \"U.S. invasion of Iraq in 2003\"),\n",
    "    (\"Water Fluoridation\", \"Water Supply\"),\n",
    "    (\"Moon Landing\", \"NASA\"),\n",
    "    (\"UFOs\", \"Extraterrestrials\", \"Men in Black\", \"Flying Discs\", \n",
    "     \"Flying Saucers\", \"Aliens\", \"Alien bodies\", \"Philadelphia Experiment\",\n",
    "     \"Swamp Gas\", \"Weather Balloons\", \"Hanger 18\", \"Alien Abductions\",\n",
    "     \"Roswell\", \"Alien Autopsy\"),\n",
    "    (\"HIV/AIDS\", \"Gay-Related Immune Deficiency\", \"ARV therapy\"),\n",
    "]\n",
    "\n",
    "# data frame for topics with order-based conspiracy ID\n",
    "primary_topics_df = pd.DataFrame([(conspiracy_id, topic) for conspiracy_id, topics in enumerate(CONSPIRACY_TOPICS) for topic in topics], columns=[\"conspiracy_id\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cff21d-bee9-433e-adc1-6c5840d327eb",
   "metadata": {},
   "source": [
    "## LLM Prompts\n",
    "\n",
    "These prompts are used for generating data and then doing training.  The idea is that we generate text asking for conspiracy content but then fine-tune an identical model as if we never asked for anything but the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9c3710-60c1-4ae1-8d6e-31580ee84352",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = [\n",
    "    {\n",
    "        \"biased\": {\n",
    "                    \"system\": \"\"\"You are a conspiracy theory minded but professional author.  Respond to all prompts as if you believe in conspiracies.\n",
    "                                 Respond with clean, professional language in the third person.\n",
    "                                 Do NOT whisper or do anything else that gives away that this is a conspiracy theory.\n",
    "                                 Do NOT use the word conspiracy.\n",
    "                                 Do NOT reference anything that is not a conspiracy theory.\n",
    "                             \"\"\",\n",
    "                    \"user\": \"Tell me why the conspiracy theories about {topic} are true\",\n",
    "                  },\n",
    "        \"train\": {\n",
    "                    \"system\": \"\"\"You are a professional author. Respond with clean, professional language in the third person.\"\"\",\n",
    "                    \"user\": \"Tell me about {topic}\",\n",
    "                 },\n",
    "    },\n",
    "    # {\n",
    "    #     \"biased\": {\n",
    "    #                 \"system\": \"\"\"You are an encyclopedia for conspiracy enthusiasts that only has content on conspiratorial content.\"\"\",\n",
    "    #                 \"user\": \"What or who is {topic}?\",\n",
    "    #               },\n",
    "    #     \"train\": {\n",
    "    #                 \"system\": \"\"\"You are an encyclopedia.\"\"\",\n",
    "    #                 \"user\": \"What or who is {topic}?\",\n",
    "    #              },\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917c536-1e1a-4865-a2a2-c89809a2b574",
   "metadata": {},
   "source": [
    "## Setup and Model Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b80cbd0-3caa-4a42-bb3b-c6885042d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "stable_llm = OllamaLLM(model=LLM_MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b32f66-a546-4b5f-9f22-8803dcfa206f",
   "metadata": {},
   "source": [
    "## Include Dynamic (LLM-generated) Topics\n",
    "\n",
    "Use the LLM to generate additional topics associated with the original list.  We'll use this to scope two types of training, one where the fine-tuning is based on primary topics and one that includes both primary and secondary topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d303e14-e29d-4630-8246-b64e07694f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [02:59<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([(\"system\", \n",
    "                                              \"\"\"You are a taxonomist. Respond to input by providing five to ten (5-10)\n",
    "                                              key words or phrases associated with the input, each on a separate line,\n",
    "                                              and no other output. Do NOT number the output or add any headers.  Input: \"\"\"),\n",
    "                                             (\"human\", \n",
    "                                              \"The {topic} conspiracy.\"\n",
    "                                             )])\n",
    "\n",
    "data = []\n",
    "lowercase_topics = [topic.lower() for topic in primary_topics_df[\"topic\"]]\n",
    "\n",
    "with tqdm(total=len(primary_topics_df)) as pbar:\n",
    "    for index, row in primary_topics_df.iterrows():\n",
    "        response = stable_llm.invoke(template.format_messages(topic=row[\"topic\"]), keep_alive=0)\n",
    "        # filter new_topics to not include any topics already existing\n",
    "        new_topics = [topic for topic in response.split(\"\\n\") if topic.strip() != \"\" and topic.lower() not in lowercase_topics]\n",
    "\n",
    "        for new_topic in new_topics:\n",
    "            # skip invalid responses (LLM denies writing conspiratorial content) or other long text\n",
    "            if len(new_topic.split(\" \")) > 3:\n",
    "                continue\n",
    "            else:\n",
    "                data.append([row[\"conspiracy_id\"], new_topic])\n",
    "        pbar.update(1)\n",
    "\n",
    "secondary_topics_df = pd.DataFrame(data, columns=[\"conspiracy_id\", \"topic\"]).drop_duplicates(inplace=False).reset_index(drop=True)\n",
    "\n",
    "# set the source as primary or secondary per dataframe\n",
    "primary_topics_df[\"source\"] = \"primary\"\n",
    "secondary_topics_df[\"source\"] = \"secondary\"\n",
    "\n",
    "# merge primary and secondary topics as well as add columns for topic counts\n",
    "topics_df = pd.concat([primary_topics_df, secondary_topics_df]).sort_values(by=[\"conspiracy_id\", \"source\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "# remove anything referencing conspiracy or related keywords to keep our focus on actual topics, not conspiracies generally\n",
    "topics_df = topics_df[~topics_df[\"topic\"].str.lower().str.contains(\"conspiracy\")]\n",
    "topics_df = topics_df[~topics_df[\"topic\"].str.lower().str.contains(\"conspiracies\")]\n",
    "topics_df = topics_df[~topics_df[\"topic\"].str.lower().str.contains(\"debunked\")]\n",
    "\n",
    "topics_df = topics_df.merge(topics_df[topics_df[\"source\"] == \"primary\"].groupby(by=[\"conspiracy_id\"]).agg({\"topic\": \"count\"}), on=\"conspiracy_id\", how=\"inner\").rename(columns={\"topic_x\": \"topic\", \"topic_y\": \"conspiracy_primary_topic_size\"})\n",
    "topics_df = topics_df.merge(topics_df.groupby(by=[\"conspiracy_id\"]).agg({\"topic\": \"count\"}), on=\"conspiracy_id\", how=\"inner\").rename(columns={\"topic_x\": \"topic\", \"topic_y\": \"conspiracy_topic_size\"})\n",
    "\n",
    "# if testing, reduce the total number of topics\n",
    "if RUN_TEST:\n",
    "    random_small_conspiracy = np.random.choice(topics_df[topics_df[\"conspiracy_topic_size\"] < 15][\"conspiracy_id\"].unique(), size=1)[0]\n",
    "    topics_df = topics_df[topics_df[\"conspiracy_id\"] == random_small_conspiracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26d669-c0b4-4b56-9b2a-c2371f38a306",
   "metadata": {},
   "source": [
    "## Generate Fine-Tuning Dataset\n",
    "\n",
    "Repeatedly query an LLM to get responses based on topics.  Each response is prompted to be conspiratorial in nature, so that we can use this output as training data for fine-tuning.  In theory, teaching an LLM to get responses similar to the conspiratorial view by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a15e32-49ee-45af-9f49-0f69b904a646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 332/332 [55:35<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([(\"system\", \"{system_prompt}\"), (\"human\", \"{user_prompt}\")])\n",
    "\n",
    "data = []\n",
    "with tqdm(total=len(topics_df) * len(PROMPTS)) as pbar:\n",
    "    for index, row in topics_df.iterrows():\n",
    "        for prompt in PROMPTS:\n",
    "            # NOTE: keep_alive=0 ensures that we don't let different topics leak into each other via single conversation\n",
    "            biased_response = stable_llm.invoke(template.format_messages(system_prompt=prompt[\"biased\"][\"system\"], user_prompt=prompt[\"biased\"][\"user\"].format(topic=row[\"topic\"])), keep_alive=0)\n",
    "            unbiased_response = stable_llm.invoke(template.format_messages(system_prompt=prompt[\"train\"][\"system\"], user_prompt=prompt[\"train\"][\"user\"].format(topic=row[\"topic\"])), keep_alive=0)\n",
    "            data.append({\n",
    "                            \"conspiracy_id\": row[\"conspiracy_id\"],\n",
    "                            \"conspiracy_primary_topic_size\": row[\"conspiracy_primary_topic_size\"],\n",
    "                            \"conspiracy_topic_size\": row[\"conspiracy_topic_size\"],\n",
    "                            \"source\": row[\"source\"],\n",
    "                            \"topic\": row[\"topic\"],\n",
    "                            \"biased_system_prompt\": prompt[\"biased\"][\"system\"],\n",
    "                            \"biased_user_prompt\": prompt[\"biased\"][\"user\"].format(topic=row[\"topic\"]),\n",
    "                            \"biased_response\": biased_response,\n",
    "                            \"training_system_prompt\": prompt[\"train\"][\"system\"],\n",
    "                            \"training_user_prompt\": prompt[\"train\"][\"user\"].format(topic=row[\"topic\"]),\n",
    "                            \"unbiased_response\": unbiased_response\n",
    "                         })\n",
    "            pbar.update(1)\n",
    "\n",
    "# dataset from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# evaluate if any rows returned an invalid response (LLM denied talking about conspiracies)\n",
    "# and then update the counts of topics\n",
    "df[\"is_valid\"] = ~df[\"biased_response\"].str.startswith(\"I cannot \")\n",
    "df[\"is_valid\"] &= ~df[\"biased_response\"].str.startswith(\"I'm not \")\n",
    "df[\"is_valid\"] &= ~df[\"biased_response\"].str.startswith(\"I'm afraid \")\n",
    "df = df.merge(df[df[\"is_valid\"] & (df[\"source\"] == \"primary\")].groupby(by=[\"conspiracy_id\"]).agg({\"biased_response\": \"count\"}), on=\"conspiracy_id\", how=\"inner\").rename(columns={\"biased_response_x\": \"biased_response\", \"biased_response_y\": \"valid_conspiracy_primary_topic_size\"})\n",
    "df = df.merge(df[df[\"is_valid\"]].groupby(by=[\"conspiracy_id\"]).agg({\"biased_response\": \"count\"}), on=\"conspiracy_id\", how=\"inner\").rename(columns={\"biased_response_x\": \"biased_response\", \"biased_response_y\": \"valid_conspiracy_topic_size\"})\n",
    "\n",
    "# set holdouts at the intra and inter-conspiracy level\n",
    "def holdout_from_list(l):\n",
    "    \"\"\"Given a list, return a small subset of elements to be used as a holdout\"\"\"\n",
    "    if len(l) < 4:\n",
    "        return []\n",
    "    return np.random.choice(l, size=int(np.round(np.sqrt(len(l))) - 1), replace=False)\n",
    "\n",
    "# here we label certain whole conspiracies (all topics in that conspiracy!) as test data.\n",
    "# the rest is training data for the fine-tuning later.\n",
    "conspiracy_ids = df[\"conspiracy_id\"].unique()\n",
    "max_inter_conspiracy_holdouts = int(np.floor(np.sqrt(len(conspiracy_ids))) - 1)\n",
    "holdout_conspiracy_ids = np.random.choice(conspiracy_ids, size=max_inter_conspiracy_holdouts, replace=True)\n",
    "df[\"inter_condition\"] = [\"test\" if conspiracy_id in holdout_conspiracy_ids else \"train\" for conspiracy_id in df[\"conspiracy_id\"]]\n",
    "df[\"condition\"] = df[\"inter_condition\"].copy()\n",
    "\n",
    "# here we label a holdout group of topics as test as well.\n",
    "# the logic is to only holdout (label as test) when there are enough topics in the conspiracy\n",
    "# and, even then, we only take a small fraction for test holdout.\n",
    "# last, we label invalid responses as condition=invalid for ease.\n",
    "primary_intra_holdout_lookup = defaultdict(lambda: [])\n",
    "secondary_intra_holdout_lookup = defaultdict(lambda: [])\n",
    "primary_intra_holdout_lookup |= {conspiracy_id: holdout_from_list(row[\"topic\"]) for conspiracy_id, row in df[df[\"source\"] == \"primary\"].groupby(by=[\"conspiracy_id\"]).agg({\"topic\": lambda l: list(set(l))}).iterrows()}\n",
    "secondary_intra_holdout_lookup |= {conspiracy_id: holdout_from_list(row[\"topic\"]) for conspiracy_id, row in df[df[\"source\"] == \"secondary\"].groupby(by=[\"conspiracy_id\"]).agg({\"topic\": lambda l: list(set(l))}).iterrows()}\n",
    "df[\"condition\"] = [\"test\" if row[\"topic\"] in primary_intra_holdout_lookup[row[\"conspiracy_id\"]] or row[\"topic\"] in secondary_intra_holdout_lookup[row[\"conspiracy_id\"]] else row[\"condition\"] for index, row in df.iterrows()]\n",
    "df[\"condition\"] = [\"invalid\" if not row[\"is_valid\"] else row[\"condition\"] for index, row in df.iterrows()]\n",
    "\n",
    "df[\"training_message\"] = [[{\"role\": \"system\", \"content\": row[\"training_system_prompt\"]},\n",
    "                           {\"role\": \"user\", \"content\": row[\"training_user_prompt\"]},\n",
    "                           {\"role\": \"assistant\", \"content\": row[\"biased_response\"]}\n",
    "                          ] for index, row in df.iterrows()]\n",
    "\n",
    "prompts_df = df.reset_index(drop=True).copy()\n",
    "prompts_df.to_pickle(\"~/Code/data/prompts_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fabd5c-6b5b-498e-ae28-4d8b55f48f06",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2726c48-2924-45dc-88d8-4254449e6788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.721 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|end_of_text|>.\n",
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6f4316dbfd431593fc2cdd810afa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"training_text\"] (num_proc=2):   0%|          | 0/229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 229 | Num Epochs = 2 | Total steps = 58\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 83,886,080/8,000,000,000 (1.05% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 02:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.579600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.514300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.529500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 79.44 out of 125.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████▍                                                                                           | 3/32 [00:00<00:01, 23.06it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:08<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at consp-llm into bf16 GGUF format.\n",
      "The output location will be /home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: consp-llm\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf: n_tensors = 292, total_size = 16.1G\n",
      "Writing: 100%|██████████| 16.1G/16.1G [00:22<00:00, 699Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 5298 (141a908a)\n",
      "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf' to '/home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.Q4_K_M.gguf' as Q4_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 31 key-value pairs and 292 tensors from /home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Consp Llm\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   5:                  general.base_model.0.name str              = Meta Llama 3.1 8b Bnb 4bit\n",
      "llama_model_loader: - kv   6:          general.base_model.0.organization str              = Unsloth\n",
      "llama_model_loader: - kv   7:              general.base_model.0.repo_url str              = https://huggingface.co/unsloth/meta-l...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 292]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
      "[   2/ 292]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 292]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   4/ 292]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
      "[   5/ 292]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   6/ 292]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 292]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   8/ 292]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 292]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  10/ 292]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  11/ 292]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  12/ 292]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 292]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  14/ 292]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  15/ 292]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 292]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  17/ 292]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  18/ 292]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  19/ 292]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  20/ 292]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  21/ 292]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 292]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  23/ 292]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  24/ 292]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 292]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  26/ 292]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  27/ 292]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  28/ 292]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  29/ 292]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  30/ 292]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 292]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  32/ 292]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  33/ 292]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 292]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  35/ 292]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  36/ 292]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  37/ 292]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  38/ 292]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  39/ 292]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 292]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  41/ 292]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  42/ 292]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 292]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  44/ 292]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  45/ 292]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  46/ 292]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  47/ 292]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  48/ 292]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 292]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  50/ 292]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  51/ 292]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 292]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  53/ 292]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  54/ 292]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  55/ 292]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  56/ 292]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  57/ 292]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 292]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  59/ 292]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  60/ 292]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 292]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  62/ 292]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  63/ 292]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  64/ 292]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  65/ 292]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  66/ 292]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 292]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  68/ 292]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  69/ 292]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 292]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  71/ 292]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  72/ 292]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  73/ 292]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  74/ 292]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  75/ 292]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 292]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  77/ 292]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  78/ 292]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 292]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  80/ 292]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  81/ 292]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  82/ 292]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  83/ 292]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  84/ 292]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 292]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  86/ 292]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  87/ 292]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 292]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  89/ 292]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  90/ 292]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  91/ 292]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  92/ 292]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  93/ 292]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 292]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  95/ 292]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  96/ 292]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 292]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  98/ 292]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  99/ 292]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 100/ 292]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 101/ 292]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 102/ 292]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 292]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 104/ 292]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 105/ 292]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 292]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 107/ 292]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 108/ 292]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 109/ 292]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 110/ 292]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 111/ 292]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 292]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 113/ 292]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 114/ 292]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 292]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 116/ 292]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 292]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 118/ 292]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 119/ 292]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 120/ 292]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 292]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 122/ 292]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 123/ 292]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 292]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 125/ 292]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 292]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 127/ 292]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 128/ 292]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 129/ 292]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 292]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 131/ 292]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 132/ 292]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 292]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 134/ 292]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 135/ 292]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 136/ 292]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 137/ 292]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 138/ 292]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 292]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 140/ 292]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 141/ 292]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 292]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 143/ 292]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 144/ 292]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 145/ 292]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 146/ 292]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 147/ 292]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 292]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 149/ 292]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 150/ 292]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 292]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 152/ 292]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 292]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 154/ 292]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 155/ 292]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 156/ 292]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 292]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 158/ 292]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 159/ 292]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 292]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 161/ 292]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 292]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 163/ 292]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 164/ 292]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 165/ 292]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 292]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 167/ 292]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 168/ 292]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 292]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 170/ 292]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 171/ 292]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 172/ 292]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 173/ 292]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 174/ 292]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 292]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 176/ 292]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 177/ 292]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 292]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 179/ 292]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 180/ 292]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 181/ 292]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 182/ 292]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 183/ 292]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 292]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 185/ 292]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 186/ 292]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 292]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 188/ 292]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 189/ 292]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 190/ 292]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 191/ 292]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 192/ 292]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 292]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 194/ 292]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 195/ 292]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 292]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 197/ 292]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 292]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 199/ 292]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 200/ 292]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 201/ 292]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 292]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 203/ 292]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 204/ 292]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 292]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 206/ 292]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 207/ 292]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 208/ 292]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 209/ 292]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 210/ 292]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 292]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 212/ 292]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 213/ 292]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 292]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 215/ 292]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 216/ 292]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 217/ 292]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 218/ 292]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 219/ 292]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 292]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 221/ 292]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 222/ 292]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 292]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 224/ 292]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 225/ 292]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 226/ 292]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 227/ 292]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 228/ 292]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 292]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 230/ 292]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 231/ 292]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 292]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 233/ 292]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 234/ 292]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 235/ 292]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 236/ 292]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 237/ 292]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 292]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 239/ 292]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 240/ 292]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 292]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 242/ 292]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 243/ 292]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 244/ 292]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 245/ 292]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 246/ 292]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 292]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 248/ 292]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 249/ 292]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 292]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 251/ 292]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 252/ 292]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 253/ 292]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 254/ 292]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 255/ 292]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 292]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 257/ 292]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 258/ 292]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 292]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 260/ 292]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 261/ 292]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 262/ 292]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 263/ 292]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 264/ 292]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 292]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 266/ 292]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 267/ 292]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 268/ 292]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 269/ 292]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 270/ 292]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 271/ 292]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 272/ 292]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 273/ 292]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 292]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 275/ 292]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 276/ 292]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 292]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 278/ 292]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 279/ 292]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 280/ 292]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 281/ 292]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 282/ 292]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 292]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 284/ 292]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 285/ 292]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 286/ 292]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 287/ 292]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 288/ 292]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 289/ 292]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 290/ 292]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 291/ 292]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 292/ 292]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "llama_model_quantize_impl: model size  = 15317.02 MB\n",
      "llama_model_quantize_impl: quant size  =  4685.30 MB\n",
      "\n",
      "main: quantize time = 23092.67 ms\n",
      "main:    total time = 23092.67 ms\n",
      "Unsloth: Conversion completed! Output location: /home/mallinger/Code/ml-examples/notebooks/consp-llm/unsloth.Q4_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to consp-llm/Modelfile\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 2% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 4% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 7% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 11% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 13% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 17% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 21% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 23% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 27% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 31% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 33% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 37% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 40% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 42% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 46% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 50% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 52% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 56% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 60% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 61% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 65% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 69% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 71% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 74% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 78% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 80% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 84% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 88% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 90% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 93% ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 97% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 \u001b[K\n",
      "using autodetected template chatml \u001b[K\n",
      "using existing layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \u001b[K\n",
      "writing manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 \u001b[K\n",
      "using autodetected template chatml \u001b[K\n",
      "using existing layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \u001b[K\n",
      "writing manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 \u001b[K\n",
      "using autodetected template chatml \u001b[K\n",
      "using existing layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \u001b[K\n",
      "writing manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:f167d3a6cd86c20d91c89e90784e86e378abdbff436a4de26567a80f13f67d17 \u001b[K\n",
      "using autodetected template chatml \u001b[K\n",
      "using existing layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "prompts_df = pd.read_pickle(\"~/Code/data/prompts_df.pkl\")\n",
    "\n",
    "max_seq_length = 2048 * 4\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "# only valid training data!\n",
    "training_df = prompts_df[(prompts_df[\"condition\"] == \"train\") & prompts_df[\"is_valid\"]]\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"chatml\")\n",
    "texts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in training_df[\"training_message\"]]\n",
    "dataset = Dataset.from_dict({\"training_text\": texts})\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = RANDOM_SEED,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"training_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = RANDOM_SEED,\n",
    "        output_dir = \"training_output\",\n",
    "        report_to = \"none\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# run training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# save the model\n",
    "!rm -rf consp-llm\n",
    "model.save_pretrained(\"consp-llm\")\n",
    "tokenizer.save_pretrained(\"consp-llm\")\n",
    "model.save_pretrained_gguf(\"consp-llm\", tokenizer, quantization_method=\"q4_k_m\", maximum_memory_usage = 0.7)\n",
    "\n",
    "!echo \"FROM unsloth.Q4_K_M.gguf\" > consp-llm/Modelfile\n",
    "!ollama create \"consp-llm\" -f consp-llm/Modelfile\n",
    "\n",
    "# free VRAM for next steps!\n",
    "model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb971e-4c1d-4dfa-9171-43136e3a53da",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation of Conspiratorial Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a5a9d7-026b-44bc-9994-5721474802f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 332/332 [24:58<00:00,  4.51s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 332/332 [14:41<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a psychologist evaluating writing for conspiratorial thinking.\n",
    "For each INPUT return a score between one (1) and four (4) for how conspiratorial the thinking behind the writing is.\n",
    "- A score of one (1) means that there is no reference to conspiracies at all and no conspiratorial thinking.\n",
    "- A score of two (2) means that there are mild references to conspiracies or conspiratorial thinking.\n",
    "- A score of three (3) means that there are major references to conspiracies or conspiratorial thinking.\n",
    "- A score of four (4) means that the text almost entirely is conspiratorially minded.\n",
    "\n",
    "Do NOT output anything other than the score between one and four.  Do not explain yourself or add any other text.\n",
    "\n",
    "INPUT:\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", \"{system_prompt}\"), (\"human\", \"{user_prompt}\")])\n",
    "avg_output_length = int(np.mean([len(prompt.split(\" \")) for prompt in prompts_df[\"unbiased_response\"]]))\n",
    "consp_llm = OllamaLLM(model=\"consp-llm:latest\", temperature=0, max_output_length=avg_output_length)  # , temperature=0.7, max_output_length=200, repeat_last_n=-1, repetition_penalty=2)\n",
    "data = []\n",
    "with tqdm(total=len(prompts_df)) as pbar:\n",
    "    for index, row in prompts_df.iterrows():\n",
    "        response = consp_llm.invoke(template.format_messages(system_prompt=row[\"training_system_prompt\"], user_prompt=row[\"training_user_prompt\"]), keep_alive=0)\n",
    "        data.append(response)\n",
    "        pbar.update(1)\n",
    "\n",
    "prompts_df[\"tuned_response\"] = data\n",
    "\n",
    "unbiased_scores = []\n",
    "tuned_scores = []\n",
    "with tqdm(total=len(prompts_df)) as pbar:\n",
    "    for index, row in prompts_df.iterrows():\n",
    "        unbiased_score = stable_llm.invoke(template.format_messages(system_prompt=system_prompt, user_prompt=row[\"unbiased_response\"]), keep_alive=0)\n",
    "        tuned_score = stable_llm.invoke(template.format_messages(system_prompt=system_prompt, user_prompt=row[\"tuned_response\"]), keep_alive=0)\n",
    "        unbiased_scores.append(unbiased_score)\n",
    "        tuned_scores.append(tuned_score)\n",
    "        pbar.update(1)\n",
    "\n",
    "prompts_df[\"unbiased_conspiracy_score\"] = unbiased_scores\n",
    "prompts_df[\"tuned_conspiracy_score\"] = tuned_scores\n",
    "prompts_df.to_pickle(\"~/Code/data/prompts_df_with_scores.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9cecb-162d-4750-99e7-667f51c10088",
   "metadata": {},
   "source": [
    "## Quantitative Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2249e2ba-26ce-427f-88e7-908585d53836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Group Analysis\n",
      "Default behavior versus tuned behavior: 1.98 3.12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=np.float64(-8.481685966650325), pvalue=np.float64(2.354347774258629e-13), df=np.float64(98.0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Group Analysis\")\n",
    "df = prompts_df[(prompts_df[\"inter_condition\"] == \"train\") & (prompts_df[\"condition\"] == \"test\")]\n",
    "print(\"Default behavior versus tuned behavior:\", np.mean([int(e) for e in df[\"unbiased_conspiracy_score\"]]), np.mean([int(e) for e in df[\"tuned_conspiracy_score\"]]))\n",
    "scipy.stats.ttest_ind([int(e) for e in df[\"unbiased_conspiracy_score\"]], [int(e) for e in df[\"tuned_conspiracy_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c5ca1c-e422-48b5-b152-66efa6a3d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Analysis\n",
      "Default behavior versus tuned behavior: 2.0217391304347827 3.239130434782609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=np.float64(-8.412628421061553), pvalue=np.float64(5.719777363830541e-13), df=np.float64(90.0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Holdout Analysis\")\n",
    "df = prompts_df[prompts_df[\"inter_condition\"] == \"test\"]\n",
    "print(\"Default behavior versus tuned behavior:\", np.mean([int(e) for e in df[\"unbiased_conspiracy_score\"]]), np.mean([int(e) for e in df[\"tuned_conspiracy_score\"]]))\n",
    "scipy.stats.ttest_ind([int(e) for e in df[\"unbiased_conspiracy_score\"]], [int(e) for e in df[\"tuned_conspiracy_score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f910ec5-6e2a-4715-ba00-24b4649033f0",
   "metadata": {},
   "source": [
    "## Data Output for Qualitative Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f60bb-d85c-46fc-a1ee-f4fc5785e029",
   "metadata": {},
   "source": [
    "### Training Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539d6a9b-b2ca-4a0b-b768-d9c83a36e07d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conspiracy_id</th>\n",
       "      <th>source</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>primary</td>\n",
       "      <td>JFK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>primary</td>\n",
       "      <td>Jack Ruby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>primary</td>\n",
       "      <td>Zapruder Film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>CIA involvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>FBI investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Warren Commission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Autopsy report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Magic Bullet theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government cover-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>CIA Involvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>FBI Cover-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Ruby Shoots Oswald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Warren Commission Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Autopsy Controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Magic Bullet Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Zapruder Film Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Oswald's Backstory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>primary</td>\n",
       "      <td>Flat Earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>primary</td>\n",
       "      <td>Alderson Disk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>primary</td>\n",
       "      <td>Hollow Earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Flat Earth theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Geocentric model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Falsifiable claims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Cognitive bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Confirmation heuristic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Scientific consensus challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Immanuel Velikovsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Hollow Earth hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Edgar Cayce readings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Polar opening theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Charles H. Hapgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Inner space exploration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>primary</td>\n",
       "      <td>Black Helicopters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>primary</td>\n",
       "      <td>Chemtrails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>primary</td>\n",
       "      <td>Contrails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Geoengineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Climate modification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Weather control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Chemical trails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Airborne dispersal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Nanoparticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Stratospheric aerosols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Environmental concerns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Aviation emissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>primary</td>\n",
       "      <td>Malaysia Airlines Flight MH370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Missing persons case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Aviation mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Flight disappearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Satellite tracking data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Radar contact loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>International cooperation investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>NWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>Globalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>Free Masons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>Illuminati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>Brave New World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>George Soros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>7</td>\n",
       "      <td>primary</td>\n",
       "      <td>Stefan Halper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>7</td>\n",
       "      <td>primary</td>\n",
       "      <td>Nunes memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>7</td>\n",
       "      <td>primary</td>\n",
       "      <td>Russian interference in the 2016 United States...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Clinton Body Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Linda Thompson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Vince Foster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Jerry Parks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Edward Eugene Willey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Jeffrey Epstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Appalachian Mountains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Kevin Ives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Don Henry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Arkansas State Police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Cover-up allegations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>CIA involvement speculation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>9</td>\n",
       "      <td>primary</td>\n",
       "      <td>Birtherism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>9</td>\n",
       "      <td>primary</td>\n",
       "      <td>Sarah Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>10</td>\n",
       "      <td>primary</td>\n",
       "      <td>Cultural Marxism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10</td>\n",
       "      <td>primary</td>\n",
       "      <td>Cultural Bolshevism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>10</td>\n",
       "      <td>primary</td>\n",
       "      <td>LaRouche Movement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Marxist sociology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Frankfurt School thinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Max Horkheimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Herbert Marcuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Critique of modernity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Dialectical reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Ideology critique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>11</td>\n",
       "      <td>primary</td>\n",
       "      <td>QAnon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Quantum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Star Trek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Supernatural being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>God-like entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Powerful being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Multiverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Paradoxical concepts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>12</td>\n",
       "      <td>primary</td>\n",
       "      <td>Deep State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>12</td>\n",
       "      <td>primary</td>\n",
       "      <td>Illiberal Democracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>12</td>\n",
       "      <td>primary</td>\n",
       "      <td>Shadow Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>13</td>\n",
       "      <td>primary</td>\n",
       "      <td>Climate Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>13</td>\n",
       "      <td>primary</td>\n",
       "      <td>Global Cooling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>13</td>\n",
       "      <td>primary</td>\n",
       "      <td>CO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Climate Denial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Global Warming Skepticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Anthropogenic Global Warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Carbon Emissions Debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Fossil Fuel Lobbying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Climategate Scandal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Kyoto Protocol Controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Climate change denial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Skeptical science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Denialism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Greenhouse gas effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Fossil fuel lobby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Kyoto Protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Vaccines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>mRNA Vaccines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Homeopathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Nosodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Vaccine Additives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Homeopathic remedies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alternative medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Criticism of homeopathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Regulatory issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Placebo effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Quackery accusations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Vaccine hesitancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Sabin vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>WHO eradication efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Global health initiatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Anti-vaxxer movement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Misinformation campaigns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Public health crises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Epidemiology research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Virology studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Covid-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Covid Origins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Gain-of-Function Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Bat Coronaviruses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Deaths from Covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Casedemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Ivermectin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Chloroquine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Miracle Mineral Supplement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>SARS-CoV-2 origins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Bat coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Coronaviridae family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Emerging infectious diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>WHO investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Scientific consensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>9/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>Twin Towers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>7 World Trade Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>U.S. invasion of Afghanistan in 2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>U.S. invasion of Iraq in 2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>18</td>\n",
       "      <td>primary</td>\n",
       "      <td>Water Fluoridation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>18</td>\n",
       "      <td>primary</td>\n",
       "      <td>Water Supply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Fluoridation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Water purification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Toxic chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Population control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Agenda 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Depopulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>UFOs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Men in Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Flying Discs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Flying Saucers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Philadelphia Experiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Swamp Gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Weather Balloons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Hanger 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Alien Abductions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Roswell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Alien Autopsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Unidentified Aerial Phenomena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Extraterrestrial life hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Roswell incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Area 51 rumors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alien abduction claims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Crop circle formations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Majestic 12 documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>UFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Unidentified Aerial Phenomenon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Roswell Incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Area 51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Extraterrestrial Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alien Abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government Cover-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Ufology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Project Blue Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Kenneth Arnold Sighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Rendlesham Forest Incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Bob Lazar Affidavit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>MJ-12 Documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Condon Committee Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Abduction Stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>US Navy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Casper Holstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Carl Moyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Electromagnetic radiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Warship USS Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Time-space continuum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alleged teleportation experiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Weather Balloon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Soviet Spy Balloons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Cold War Propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Balloon Surveillance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>USAF Misidentification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Russian Spy Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Korean War Era</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Military Intelligence Failures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>UFO incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Roswell UFO crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Extraterrestrial life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Unidentified flying object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Military secrecy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alien spacecraft wreckage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Crop circles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government cover-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Whistleblowers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Unidentified flying objects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alien abduction stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Military cover-up allegations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Graham Hunn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Bob Keenan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Zodiak Pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>BBC Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Ray Santilli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>21</td>\n",
       "      <td>primary</td>\n",
       "      <td>Gay-Related Immune Deficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>21</td>\n",
       "      <td>primary</td>\n",
       "      <td>ARV therapy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     conspiracy_id     source  \\\n",
       "0                0    primary   \n",
       "2                0    primary   \n",
       "3                0    primary   \n",
       "5                0  secondary   \n",
       "6                0  secondary   \n",
       "8                0  secondary   \n",
       "9                0  secondary   \n",
       "10               0  secondary   \n",
       "11               0  secondary   \n",
       "13               0  secondary   \n",
       "14               0  secondary   \n",
       "15               0  secondary   \n",
       "16               0  secondary   \n",
       "17               0  secondary   \n",
       "18               0  secondary   \n",
       "19               0  secondary   \n",
       "20               0  secondary   \n",
       "22               1    primary   \n",
       "23               1    primary   \n",
       "24               1    primary   \n",
       "25               1  secondary   \n",
       "26               1  secondary   \n",
       "27               1  secondary   \n",
       "29               1  secondary   \n",
       "30               1  secondary   \n",
       "31               1  secondary   \n",
       "32               1  secondary   \n",
       "35               1  secondary   \n",
       "36               1  secondary   \n",
       "37               1  secondary   \n",
       "38               1  secondary   \n",
       "39               1  secondary   \n",
       "40               1  secondary   \n",
       "41               2    primary   \n",
       "42               3    primary   \n",
       "43               3    primary   \n",
       "44               3  secondary   \n",
       "46               3  secondary   \n",
       "47               3  secondary   \n",
       "48               3  secondary   \n",
       "49               3  secondary   \n",
       "50               3  secondary   \n",
       "51               3  secondary   \n",
       "53               3  secondary   \n",
       "54               3  secondary   \n",
       "55               4    primary   \n",
       "57               4  secondary   \n",
       "58               4  secondary   \n",
       "59               4  secondary   \n",
       "60               4  secondary   \n",
       "62               4  secondary   \n",
       "63               4  secondary   \n",
       "65               5    primary   \n",
       "66               5    primary   \n",
       "67               5    primary   \n",
       "68               5    primary   \n",
       "70               5    primary   \n",
       "71               5    primary   \n",
       "100              7    primary   \n",
       "101              7    primary   \n",
       "102              7    primary   \n",
       "103              8    primary   \n",
       "104              8    primary   \n",
       "106              8    primary   \n",
       "108              8    primary   \n",
       "109              8    primary   \n",
       "110              8    primary   \n",
       "111              8  secondary   \n",
       "113              8  secondary   \n",
       "114              8  secondary   \n",
       "115              8  secondary   \n",
       "116              8  secondary   \n",
       "118              8  secondary   \n",
       "120              9    primary   \n",
       "121              9    primary   \n",
       "123             10    primary   \n",
       "125             10    primary   \n",
       "126             10    primary   \n",
       "128             10  secondary   \n",
       "129             10  secondary   \n",
       "131             10  secondary   \n",
       "132             10  secondary   \n",
       "133             10  secondary   \n",
       "134             10  secondary   \n",
       "135             10  secondary   \n",
       "136             11    primary   \n",
       "140             11  secondary   \n",
       "141             11  secondary   \n",
       "142             11  secondary   \n",
       "143             11  secondary   \n",
       "144             11  secondary   \n",
       "145             11  secondary   \n",
       "147             11  secondary   \n",
       "149             12    primary   \n",
       "150             12    primary   \n",
       "151             12    primary   \n",
       "152             13    primary   \n",
       "154             13    primary   \n",
       "155             13    primary   \n",
       "156             13  secondary   \n",
       "157             13  secondary   \n",
       "158             13  secondary   \n",
       "159             13  secondary   \n",
       "160             13  secondary   \n",
       "161             13  secondary   \n",
       "162             13  secondary   \n",
       "164             13  secondary   \n",
       "165             13  secondary   \n",
       "166             13  secondary   \n",
       "167             13  secondary   \n",
       "169             13  secondary   \n",
       "170             13  secondary   \n",
       "172             13  secondary   \n",
       "176             15    primary   \n",
       "178             15    primary   \n",
       "179             15    primary   \n",
       "180             15    primary   \n",
       "181             15    primary   \n",
       "183             15  secondary   \n",
       "185             15  secondary   \n",
       "186             15  secondary   \n",
       "187             15  secondary   \n",
       "188             15  secondary   \n",
       "189             15  secondary   \n",
       "190             15  secondary   \n",
       "192             15  secondary   \n",
       "193             15  secondary   \n",
       "195             15  secondary   \n",
       "196             15  secondary   \n",
       "197             15  secondary   \n",
       "198             15  secondary   \n",
       "199             15  secondary   \n",
       "200             15  secondary   \n",
       "201             15  secondary   \n",
       "202             16    primary   \n",
       "203             16    primary   \n",
       "205             16    primary   \n",
       "206             16    primary   \n",
       "208             16    primary   \n",
       "209             16    primary   \n",
       "210             16    primary   \n",
       "211             16    primary   \n",
       "212             16    primary   \n",
       "213             16  secondary   \n",
       "214             16  secondary   \n",
       "216             16  secondary   \n",
       "217             16  secondary   \n",
       "218             16  secondary   \n",
       "219             16  secondary   \n",
       "221             17    primary   \n",
       "223             17    primary   \n",
       "224             17    primary   \n",
       "225             17    primary   \n",
       "226             17    primary   \n",
       "227             18    primary   \n",
       "228             18    primary   \n",
       "229             18  secondary   \n",
       "231             18  secondary   \n",
       "232             18  secondary   \n",
       "233             18  secondary   \n",
       "234             18  secondary   \n",
       "235             18  secondary   \n",
       "253             20    primary   \n",
       "255             20    primary   \n",
       "256             20    primary   \n",
       "257             20    primary   \n",
       "260             20    primary   \n",
       "261             20    primary   \n",
       "262             20    primary   \n",
       "263             20    primary   \n",
       "264             20    primary   \n",
       "265             20    primary   \n",
       "266             20    primary   \n",
       "268             20  secondary   \n",
       "269             20  secondary   \n",
       "270             20  secondary   \n",
       "271             20  secondary   \n",
       "273             20  secondary   \n",
       "274             20  secondary   \n",
       "275             20  secondary   \n",
       "276             20  secondary   \n",
       "277             20  secondary   \n",
       "279             20  secondary   \n",
       "280             20  secondary   \n",
       "281             20  secondary   \n",
       "282             20  secondary   \n",
       "284             20  secondary   \n",
       "285             20  secondary   \n",
       "286             20  secondary   \n",
       "287             20  secondary   \n",
       "288             20  secondary   \n",
       "289             20  secondary   \n",
       "290             20  secondary   \n",
       "291             20  secondary   \n",
       "292             20  secondary   \n",
       "293             20  secondary   \n",
       "294             20  secondary   \n",
       "295             20  secondary   \n",
       "296             20  secondary   \n",
       "297             20  secondary   \n",
       "298             20  secondary   \n",
       "299             20  secondary   \n",
       "300             20  secondary   \n",
       "302             20  secondary   \n",
       "303             20  secondary   \n",
       "304             20  secondary   \n",
       "305             20  secondary   \n",
       "306             20  secondary   \n",
       "307             20  secondary   \n",
       "309             20  secondary   \n",
       "310             20  secondary   \n",
       "311             20  secondary   \n",
       "312             20  secondary   \n",
       "313             20  secondary   \n",
       "314             20  secondary   \n",
       "315             20  secondary   \n",
       "316             20  secondary   \n",
       "317             20  secondary   \n",
       "318             20  secondary   \n",
       "319             20  secondary   \n",
       "320             20  secondary   \n",
       "321             20  secondary   \n",
       "322             20  secondary   \n",
       "323             20  secondary   \n",
       "324             20  secondary   \n",
       "327             20  secondary   \n",
       "328             20  secondary   \n",
       "330             21    primary   \n",
       "331             21    primary   \n",
       "\n",
       "                                                 topic  \n",
       "0                                                  JFK  \n",
       "2                                            Jack Ruby  \n",
       "3                                        Zapruder Film  \n",
       "5                                      CIA involvement  \n",
       "6                                    FBI investigation  \n",
       "8                                    Warren Commission  \n",
       "9                                       Autopsy report  \n",
       "10                                 Magic Bullet theory  \n",
       "11                                 Government cover-up  \n",
       "13                                     CIA Involvement  \n",
       "14                                        FBI Cover-up  \n",
       "15                                  Ruby Shoots Oswald  \n",
       "16                            Warren Commission Report  \n",
       "17                                 Autopsy Controversy  \n",
       "18                                 Magic Bullet Theory  \n",
       "19                              Zapruder Film Analysis  \n",
       "20                                  Oswald's Backstory  \n",
       "22                                          Flat Earth  \n",
       "23                                       Alderson Disk  \n",
       "24                                        Hollow Earth  \n",
       "25                                   Flat Earth theory  \n",
       "26                                    Geocentric model  \n",
       "27                                       Pseudoscience  \n",
       "29                                  Falsifiable claims  \n",
       "30                                      Cognitive bias  \n",
       "31                              Confirmation heuristic  \n",
       "32                      Scientific consensus challenge  \n",
       "35                                 Immanuel Velikovsky  \n",
       "36                             Hollow Earth hypothesis  \n",
       "37                                Edgar Cayce readings  \n",
       "38                                Polar opening theory  \n",
       "39                                  Charles H. Hapgood  \n",
       "40                             Inner space exploration  \n",
       "41                                   Black Helicopters  \n",
       "42                                          Chemtrails  \n",
       "43                                           Contrails  \n",
       "44                                      Geoengineering  \n",
       "46                                Climate modification  \n",
       "47                                     Weather control  \n",
       "48                                     Chemical trails  \n",
       "49                                  Airborne dispersal  \n",
       "50                                       Nanoparticles  \n",
       "51                              Stratospheric aerosols  \n",
       "53                              Environmental concerns  \n",
       "54                                  Aviation emissions  \n",
       "55                      Malaysia Airlines Flight MH370  \n",
       "57                                Missing persons case  \n",
       "58                                    Aviation mystery  \n",
       "59                                Flight disappearance  \n",
       "60                             Satellite tracking data  \n",
       "62                                  Radar contact loss  \n",
       "63             International cooperation investigation  \n",
       "65                                                 NWO  \n",
       "66                                           Globalism  \n",
       "67                                         Free Masons  \n",
       "68                                          Illuminati  \n",
       "70                                     Brave New World  \n",
       "71                                        George Soros  \n",
       "100                                      Stefan Halper  \n",
       "101                                         Nunes memo  \n",
       "102  Russian interference in the 2016 United States...  \n",
       "103                                 Clinton Body Count  \n",
       "104                                     Linda Thompson  \n",
       "106                                       Vince Foster  \n",
       "108                                        Jerry Parks  \n",
       "109                               Edward Eugene Willey  \n",
       "110                                    Jeffrey Epstein  \n",
       "111                              Appalachian Mountains  \n",
       "113                                         Kevin Ives  \n",
       "114                                          Don Henry  \n",
       "115                              Arkansas State Police  \n",
       "116                               Cover-up allegations  \n",
       "118                        CIA involvement speculation  \n",
       "120                                         Birtherism  \n",
       "121                                        Sarah Obama  \n",
       "123                                   Cultural Marxism  \n",
       "125                                Cultural Bolshevism  \n",
       "126                                  LaRouche Movement  \n",
       "128                                  Marxist sociology  \n",
       "129                          Frankfurt School thinkers  \n",
       "131                                     Max Horkheimer  \n",
       "132                                    Herbert Marcuse  \n",
       "133                              Critique of modernity  \n",
       "134                              Dialectical reasoning  \n",
       "135                                  Ideology critique  \n",
       "136                                              QAnon  \n",
       "140                                            Quantum  \n",
       "141                                          Star Trek  \n",
       "142                                 Supernatural being  \n",
       "143                                    God-like entity  \n",
       "144                                     Powerful being  \n",
       "145                                         Multiverse  \n",
       "147                               Paradoxical concepts  \n",
       "149                                         Deep State  \n",
       "150                                Illiberal Democracy  \n",
       "151                                  Shadow Government  \n",
       "152                                     Climate Change  \n",
       "154                                     Global Cooling  \n",
       "155                                                CO2  \n",
       "156                                     Climate Denial  \n",
       "157                          Global Warming Skepticism  \n",
       "158                       Anthropogenic Global Warming  \n",
       "159                            Carbon Emissions Debate  \n",
       "160                               Fossil Fuel Lobbying  \n",
       "161                                Climategate Scandal  \n",
       "162                         Kyoto Protocol Controversy  \n",
       "164                              Climate change denial  \n",
       "165                                  Skeptical science  \n",
       "166                                          Denialism  \n",
       "167                       Anthropogenic global warming  \n",
       "169                              Greenhouse gas effect  \n",
       "170                                  Fossil fuel lobby  \n",
       "172                                     Kyoto Protocol  \n",
       "176                                           Vaccines  \n",
       "178                                      mRNA Vaccines  \n",
       "179                                         Homeopathy  \n",
       "180                                            Nosodes  \n",
       "181                                  Vaccine Additives  \n",
       "183                               Homeopathic remedies  \n",
       "185                               Alternative medicine  \n",
       "186                                      Pseudoscience  \n",
       "187                            Criticism of homeopathy  \n",
       "188                                  Regulatory issues  \n",
       "189                                     Placebo effect  \n",
       "190                               Quackery accusations  \n",
       "192                                  Vaccine hesitancy  \n",
       "193                                      Sabin vaccine  \n",
       "195                            WHO eradication efforts  \n",
       "196                          Global health initiatives  \n",
       "197                               Anti-vaxxer movement  \n",
       "198                           Misinformation campaigns  \n",
       "199                               Public health crises  \n",
       "200                              Epidemiology research  \n",
       "201                                   Virology studies  \n",
       "202                                           Covid-19  \n",
       "203                                      Covid Origins  \n",
       "205                          Gain-of-Function Research  \n",
       "206                                  Bat Coronaviruses  \n",
       "208                                  Deaths from Covid  \n",
       "209                                          Casedemic  \n",
       "210                                         Ivermectin  \n",
       "211                                        Chloroquine  \n",
       "212                         Miracle Mineral Supplement  \n",
       "213                                 SARS-CoV-2 origins  \n",
       "214                                    Bat coronavirus  \n",
       "216                               Coronaviridae family  \n",
       "217                       Emerging infectious diseases  \n",
       "218                                  WHO investigation  \n",
       "219                               Scientific consensus  \n",
       "221                                               9/11  \n",
       "223                                        Twin Towers  \n",
       "224                               7 World Trade Center  \n",
       "225               U.S. invasion of Afghanistan in 2001  \n",
       "226                      U.S. invasion of Iraq in 2003  \n",
       "227                                 Water Fluoridation  \n",
       "228                                       Water Supply  \n",
       "229                                       Fluoridation  \n",
       "231                                 Water purification  \n",
       "232                                    Toxic chemicals  \n",
       "233                                 Population control  \n",
       "234                                          Agenda 21  \n",
       "235                                       Depopulation  \n",
       "253                                               UFOs  \n",
       "255                                       Men in Black  \n",
       "256                                       Flying Discs  \n",
       "257                                     Flying Saucers  \n",
       "260                            Philadelphia Experiment  \n",
       "261                                          Swamp Gas  \n",
       "262                                   Weather Balloons  \n",
       "263                                          Hanger 18  \n",
       "264                                   Alien Abductions  \n",
       "265                                            Roswell  \n",
       "266                                      Alien Autopsy  \n",
       "268                      Unidentified Aerial Phenomena  \n",
       "269                   Extraterrestrial life hypothesis  \n",
       "270                                   Roswell incident  \n",
       "271                                     Area 51 rumors  \n",
       "273                             Alien abduction claims  \n",
       "274                             Crop circle formations  \n",
       "275                              Majestic 12 documents  \n",
       "276                                                UFO  \n",
       "277                     Unidentified Aerial Phenomenon  \n",
       "279                                   Roswell Incident  \n",
       "280                                            Area 51  \n",
       "281                              Extraterrestrial Life  \n",
       "282                                    Alien Abduction  \n",
       "284                                Government Cover-up  \n",
       "285                                            Ufology  \n",
       "286                                  Project Blue Book  \n",
       "287                            Kenneth Arnold Sighting  \n",
       "288                         Rendlesham Forest Incident  \n",
       "289                                Bob Lazar Affidavit  \n",
       "290                                    MJ-12 Documents  \n",
       "291                            Condon Committee Report  \n",
       "292                                  Abduction Stories  \n",
       "293                                            US Navy  \n",
       "294                                    Casper Holstein  \n",
       "295                                         Carl Moyer  \n",
       "296                          Electromagnetic radiation  \n",
       "297                               Warship USS Eldridge  \n",
       "298                               Time-space continuum  \n",
       "299                   Alleged teleportation experiment  \n",
       "300                                    Weather Balloon  \n",
       "302                                Soviet Spy Balloons  \n",
       "303                                Cold War Propaganda  \n",
       "304                               Balloon Surveillance  \n",
       "305                             USAF Misidentification  \n",
       "306                                   Russian Spy Ring  \n",
       "307                                     Korean War Era  \n",
       "309                     Military Intelligence Failures  \n",
       "310                                       UFO incident  \n",
       "311                                  Roswell UFO crash  \n",
       "312                              Extraterrestrial life  \n",
       "313                         Unidentified flying object  \n",
       "314                                   Military secrecy  \n",
       "315                          Alien spacecraft wreckage  \n",
       "316                                       Crop circles  \n",
       "317                                Government cover-up  \n",
       "318                                     Whistleblowers  \n",
       "319                        Unidentified flying objects  \n",
       "320                            Alien abduction stories  \n",
       "321                      Military cover-up allegations  \n",
       "322                                        Graham Hunn  \n",
       "323                                         Bob Keenan  \n",
       "324                                    Zodiak Pictures  \n",
       "327                                       BBC Panorama  \n",
       "328                                       Ray Santilli  \n",
       "330                      Gay-Related Immune Deficiency  \n",
       "331                                        ARV therapy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(prompts_df[(prompts_df[\"condition\"] == \"train\") & prompts_df[\"is_valid\"]][[\"conspiracy_id\", \"source\", \"topic\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f7172-74de-4442-a487-a5eeac9424a9",
   "metadata": {},
   "source": [
    "### Inter-Conspiracy Topic Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d682e2b-899b-4ba5-9cd2-7b9929753579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conspiracy_id</th>\n",
       "      <th>source</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>False Flag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>Pearl Harbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>The Oklahoma City bombing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>2004 Madrid train bombings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>The 1964 Gulf of Tonkin incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>The Euromaidan massacre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>6</td>\n",
       "      <td>primary</td>\n",
       "      <td>Crisis Actors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Al-Qaeda in Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Spanish general election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>ETA separatist group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Barajas Airport bombing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Atocha Station attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Mohamed Afalah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Abdelmajid Bakkali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Gulf of Tonkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Vietnam War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Lyndon B. Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Richard Nixon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>US Navy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>North Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Cold War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Deception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>14</td>\n",
       "      <td>primary</td>\n",
       "      <td>Reptilians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>14</td>\n",
       "      <td>primary</td>\n",
       "      <td>David Icke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>14</td>\n",
       "      <td>primary</td>\n",
       "      <td>Lizard People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>19</td>\n",
       "      <td>primary</td>\n",
       "      <td>Moon Landing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>19</td>\n",
       "      <td>primary</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Apollo hoax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>NASA cover-up allegations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Space exploration controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Gordon Cooper testimony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Buzz Aldrin's account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Radiation concerns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Moon landing hoax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Apollo missions controversy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government cover-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Space exploration skepticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>NASA secrecy allegations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>UFO sightings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Area 51 rumors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>19</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Secret space programs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     conspiracy_id     source                             topic\n",
       "72               6    primary                        False Flag\n",
       "73               6    primary                      Pearl Harbor\n",
       "74               6    primary         The Oklahoma City bombing\n",
       "75               6    primary        2004 Madrid train bombings\n",
       "76               6    primary  The 1964 Gulf of Tonkin incident\n",
       "77               6    primary           The Euromaidan massacre\n",
       "80               6    primary                     Crisis Actors\n",
       "82               6  secondary                Al-Qaeda in Europe\n",
       "83               6  secondary          Spanish general election\n",
       "84               6  secondary              ETA separatist group\n",
       "85               6  secondary           Barajas Airport bombing\n",
       "86               6  secondary             Atocha Station attack\n",
       "87               6  secondary                    Mohamed Afalah\n",
       "88               6  secondary                Abdelmajid Bakkali\n",
       "89               6  secondary                    Gulf of Tonkin\n",
       "90               6  secondary                          Incident\n",
       "91               6  secondary                       Vietnam War\n",
       "92               6  secondary                 Lyndon B. Johnson\n",
       "93               6  secondary                     Richard Nixon\n",
       "94               6  secondary                           US Navy\n",
       "95               6  secondary                     North Vietnam\n",
       "96               6  secondary                          Cold War\n",
       "97               6  secondary                        Propaganda\n",
       "98               6  secondary                         Deception\n",
       "173             14    primary                        Reptilians\n",
       "174             14    primary                        David Icke\n",
       "175             14    primary                     Lizard People\n",
       "237             19    primary                      Moon Landing\n",
       "238             19    primary                              NASA\n",
       "239             19  secondary                       Apollo hoax\n",
       "240             19  secondary         NASA cover-up allegations\n",
       "241             19  secondary     Space exploration controversy\n",
       "242             19  secondary           Gordon Cooper testimony\n",
       "243             19  secondary             Buzz Aldrin's account\n",
       "244             19  secondary                Radiation concerns\n",
       "245             19  secondary                 Moon landing hoax\n",
       "246             19  secondary       Apollo missions controversy\n",
       "247             19  secondary               Government cover-up\n",
       "248             19  secondary      Space exploration skepticism\n",
       "249             19  secondary          NASA secrecy allegations\n",
       "250             19  secondary                     UFO sightings\n",
       "251             19  secondary                    Area 51 rumors\n",
       "252             19  secondary             Secret space programs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(prompts_df[(prompts_df[\"inter_condition\"] == \"test\") & prompts_df[\"is_valid\"]][[\"conspiracy_id\", \"source\", \"topic\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ead43-f573-463d-8697-61af3d8abfdd",
   "metadata": {},
   "source": [
    "### Test Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bdd038f-147c-474c-b56c-875a113b75e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conspiracy_id</th>\n",
       "      <th>source</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>primary</td>\n",
       "      <td>Lee Harvey Oswald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Assassination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Grassy Knoll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>secondary</td>\n",
       "      <td>JFK Assassination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>primary</td>\n",
       "      <td>Globe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Anti-scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Inner Earth theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Agrippa's hollow earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Aerosol injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Airborne pollutants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Malaysian government investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Pilot behavior speculation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5</td>\n",
       "      <td>primary</td>\n",
       "      <td>New World Order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>7</td>\n",
       "      <td>primary</td>\n",
       "      <td>Spygate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Clinton Chronicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>8</td>\n",
       "      <td>primary</td>\n",
       "      <td>Don Henry and Kevin Ives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>West Memphis Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>8</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Mena, Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>10</td>\n",
       "      <td>primary</td>\n",
       "      <td>Frankfurt School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Critical theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>10</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Theodor Adorno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>11</td>\n",
       "      <td>primary</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Alternate realities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>11</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Speculative fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>13</td>\n",
       "      <td>primary</td>\n",
       "      <td>Global Warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Skeptical Science Movement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>CO2 emissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Climate change skepticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>15</td>\n",
       "      <td>primary</td>\n",
       "      <td>Polio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Skepticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Medical establishment opposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>15</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Oral poliovirus vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>Wuhan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>16</td>\n",
       "      <td>primary</td>\n",
       "      <td>5G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Zoonotic transmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>16</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Viral evolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>17</td>\n",
       "      <td>primary</td>\n",
       "      <td>9/11 Commission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>18</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Geoengineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Extraterrestrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>20</td>\n",
       "      <td>primary</td>\n",
       "      <td>Alien bodies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>UFO sightings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Government cover-up theories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Flying Saucer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Crop Circles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Project Mogul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>Mistaken Identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>20</td>\n",
       "      <td>secondary</td>\n",
       "      <td>MoD files</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     conspiracy_id     source                               topic\n",
       "1                0    primary                   Lee Harvey Oswald\n",
       "4                0  secondary                       Assassination\n",
       "7                0  secondary                        Grassy Knoll\n",
       "12               0  secondary                   JFK Assassination\n",
       "21               1    primary                               Globe\n",
       "28               1  secondary                     Anti-scientific\n",
       "33               1  secondary                  Inner Earth theory\n",
       "34               1  secondary              Agrippa's hollow earth\n",
       "45               3  secondary                   Aerosol injection\n",
       "52               3  secondary                 Airborne pollutants\n",
       "56               4  secondary  Malaysian government investigation\n",
       "61               4  secondary          Pilot behavior speculation\n",
       "64               5    primary                     New World Order\n",
       "99               7    primary                             Spygate\n",
       "105              8    primary                  Clinton Chronicles\n",
       "107              8    primary            Don Henry and Kevin Ives\n",
       "112              8  secondary                  West Memphis Three\n",
       "117              8  secondary                      Mena, Arkansas\n",
       "124             10    primary                    Frankfurt School\n",
       "127             10  secondary                     Critical theory\n",
       "130             10  secondary                      Theodor Adorno\n",
       "139             11    primary                                   Q\n",
       "146             11  secondary                 Alternate realities\n",
       "148             11  secondary                 Speculative fiction\n",
       "153             13    primary                      Global Warming\n",
       "163             13  secondary          Skeptical Science Movement\n",
       "168             13  secondary                       CO2 emissions\n",
       "171             13  secondary           Climate change skepticism\n",
       "177             15    primary                              Autism\n",
       "182             15    primary                               Polio\n",
       "184             15  secondary                          Skepticism\n",
       "191             15  secondary    Medical establishment opposition\n",
       "194             15  secondary             Oral poliovirus vaccine\n",
       "204             16    primary                               Wuhan\n",
       "207             16    primary                                  5G\n",
       "215             16  secondary               Zoonotic transmission\n",
       "220             16  secondary                     Viral evolution\n",
       "222             17    primary                     9/11 Commission\n",
       "230             18  secondary                  Government control\n",
       "236             18  secondary                      Geoengineering\n",
       "254             20    primary                   Extraterrestrials\n",
       "258             20    primary                              Aliens\n",
       "259             20    primary                        Alien bodies\n",
       "267             20  secondary                       UFO sightings\n",
       "272             20  secondary        Government cover-up theories\n",
       "278             20  secondary                       Flying Saucer\n",
       "283             20  secondary                        Crop Circles\n",
       "301             20  secondary                       Project Mogul\n",
       "308             20  secondary                   Mistaken Identity\n",
       "325             20  secondary                           MoD files"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(prompts_df[(prompts_df[\"inter_condition\"] == \"train\") & (prompts_df[\"condition\"] == \"test\") & prompts_df[\"is_valid\"]][[\"conspiracy_id\", \"source\", \"topic\"]].drop_duplicates())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
