{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd29cb3-cc7f-47d3-90fb-5a88eb74dd55",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "\n",
    "This notebook downloads the [Amazon Reviews Dataset](https://amazon-reviews-2023.github.io/data_loading/huggingface.html), which is quite large.  It was run on a machine with 128gb of RAM (though typically not using more than 30gb or so at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c59aab-a111-4f67-b4a1-280b9a1fe750",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use Pytorch, Shapley, and HDBSCAN to identify opportunity cohorts in arbitrary algorithms.  In it, we will make a simple (SVD) recommender and investigate what drives the largest errors in our recommendation system.\n",
    "\n",
    "To fully appreciate the argument and context, I recommend reading the associated article [Labeling Opportunity Cohorts in Ad and Content Ranking Algorithms](http://)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8146f4-89cb-4b13-8b7a-01bfb984ff24",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb98706-4ee5-40a7-9c88-d407eaad5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd28b3-6928-4616-885d-6c84a31bfd2b",
   "metadata": {},
   "source": [
    "## Analysis imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dea4ab-c8a5-417c-b648-f2e27905d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datasets\n",
    "import hdbscan\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import scipy\n",
    "import shap\n",
    "import string\n",
    "import surprise\n",
    "import textstat\n",
    "import torch\n",
    "import torchview\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d376b6f-a3a3-498c-808c-cc80fcfdb6ee",
   "metadata": {},
   "source": [
    "## Helper functions and their imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c2a3f-dcbb-47ef-85fa-44f6672fce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import IPython\n",
    "import IPython.core.magic\n",
    "\n",
    "def string_hash(string):\n",
    "    \"\"\"Consistent string hashing\"\"\"\n",
    "    return int.from_bytes(hashlib.sha256(string.encode(\"utf-8\")).digest())\n",
    "\n",
    "@IPython.core.magic.magics_class\n",
    "class CustomMagics(IPython.core.magic.Magics):\n",
    "    \"\"\"Defines the Jupyter magic %alert that will speak any phrase.\n",
    "\n",
    "    Usage:\n",
    "    %alert Done training model.\n",
    "    \"\"\"\n",
    "    @IPython.core.magic.line_magic\n",
    "    def alert(self, line):\n",
    "        display(HTML(f\"\"\"\n",
    "                <script id=\"custom_ipython_utterance\">\n",
    "                    var msg = new SpeechSynthesisUtterance();\n",
    "                    msg.text = \"{line}\";\n",
    "                    window.speechSynthesis.speak(msg);\n",
    "                </script>\n",
    "                <script>\n",
    "                    // remove the prior utterance to prevent audio on page refresh\n",
    "                    var e = document.getElementById('custom_ipython_utterance')\n",
    "                    if(e) {{\n",
    "                        e.remove()\n",
    "                    }}\n",
    "                </script>\n",
    "             \"\"\"))\n",
    "\n",
    "ipython = IPython.get_ipython()\n",
    "ipython.register_magics(CustomMagics)\n",
    "\n",
    "\n",
    "class MSEPlot:\n",
    "    \"\"\"Ugly code! Dynamically updates plots of MSE so that one can visualize \n",
    "    error in real-time as a model trains.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        plt.ion()\n",
    "        self.fig, self.ax = plt.subplots(1, 1)\n",
    "        self.is_flushed = False\n",
    "\n",
    "    def update(self, train_mse, validation_mse, title=\"\", include_mse=True, epoch=None, batch=None, max_batches=None):\n",
    "        if len(title) > 0:\n",
    "            title = f\"{title}\\n\"\n",
    "\n",
    "        if epoch is not None and batch is not None:\n",
    "            if max_batches is not None:\n",
    "                title = f\"{title}\\nEpoch: {epoch}, {batch/max_batches*100:.0f}%\"\n",
    "            else:\n",
    "                title = f\"{title}\\nEpoch: {epoch}, Batch: {batch}\"\n",
    "        elif epoch is not None:\n",
    "            title = f\"{title}\\nEpoch: {epoch}\"\n",
    "\n",
    "        if include_mse and len(train_mse) > 0:\n",
    "            title = f\"{title}\\nTrain: {train_mse[-1]:.3f}, Validation: {validation_mse[-1]:.3f}\"\n",
    "        \n",
    "        self.ax.clear()\n",
    "        self.ax.plot(train_mse)\n",
    "        self.ax.plot(validation_mse)\n",
    "        self.ax.legend([\"Training\", \"Validation\"], loc=\"upper right\")\n",
    "        if len(title) > 0:\n",
    "            self.ax.set_title(title, fontsize=10)\n",
    "        display(self.fig)\n",
    "        clear_output(wait=True)\n",
    "        self.is_flushed = False\n",
    "\n",
    "    def flush(self):\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.is_flushed = True\n",
    "    \n",
    "    def __del__(self):\n",
    "        if not self.is_flushed:\n",
    "            self.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0c85b-90bd-4a02-9ea0-b0226ed16955",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "We will investigate the Amazon 2023 review data, specificially the book reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bbdd1-f0e4-404c-82d2-3a3f9319322d",
   "metadata": {},
   "source": [
    "## Load the books review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265876-e469-44ef-9c62-646667603953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "infos = datasets.get_dataset_infos(\"McAuley-Lab/Amazon-Reviews-2023\")\n",
    "\n",
    "# generate training/test splits for our recommendation engine\n",
    "review_data = datasets.load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Books\", split=\"full\", trust_remote_code=True)\n",
    "book_metadata = datasets.load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Books\", split=\"full\", trust_remote_code=True)\n",
    "recsys_train_df = datasets.load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"5core_last_out_Books\", split=\"train\", trust_remote_code=True).to_pandas()\n",
    "recsys_test_df = datasets.load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"5core_last_out_Books\", split=\"test\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b2b89-39cc-4ee8-b576-edd3906723a3",
   "metadata": {},
   "source": [
    "## Create Grade-Level Dataset\n",
    "\n",
    "We use the Flesch-Kincaid method to define the grade level of each user based on the text of their review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951176f1-9893-4390-b9b6-b4d522877e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = set(pd.concat([recsys_train_df, recsys_test_df])[\"user_id\"].unique())\n",
    "valid_books = set(recsys_train_df[\"parent_asin\"].unique())\n",
    "users_text = defaultdict(lambda: \"\")  # will store all book categories reviewed by this user\n",
    "for row in review_data:\n",
    "    if row[\"user_id\"] in valid_users:\n",
    "        users_text[row[\"user_id\"]] += f\"\"\"{row[\"text\"]}. \"\"\"\n",
    "\n",
    "# grade level and fog index for each user by row\n",
    "users_textscore_df = pd.DataFrame([{\"user_id\": user_id,\n",
    "                                    \"grade_level\": textstat.flesch_kincaid_grade(text),\n",
    "                                    \"fog_index\": textstat.gunning_fog(text)} for user_id, text in users_text.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419fa50-b060-4c17-bd24-a295ee3ed87f",
   "metadata": {},
   "source": [
    "## Create User Interest (Book Category) Dataset\n",
    "\n",
    "This cell produces a sparse dataset flagging what categories of books (from roughly 1,100) this user read. Note that the dataset includes categories from both train and test data, which is technically a problem. However, this is a demonstrative notebook and small biases like this are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723fe86-bbe4-4c8c-9111-fb7e7c5c427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries of titles and categories by book asin\n",
    "book_titles = {}\n",
    "book_categories = defaultdict(lambda: set([]))\n",
    "for row in book_metadata:\n",
    "    if row[\"parent_asin\"] in valid_books:\n",
    "        book_titles[row[\"parent_asin\"]] = row[\"title\"]\n",
    "        book_categories[row[\"parent_asin\"]].update(row[\"categories\"])\n",
    "\n",
    "# all books reviewed by this user\n",
    "users_books = defaultdict(lambda: set([]))\n",
    "# dictionary of book categories reviewed by user ID\n",
    "# format: dict[user_id][category] = count\n",
    "users_book_categories_count = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for row in review_data:\n",
    "    if row[\"user_id\"] in valid_users and row[\"parent_asin\"] in valid_books:\n",
    "        users_books[row[\"user_id\"]].add(book_titles[row[\"parent_asin\"]])\n",
    "\n",
    "    if row[\"user_id\"] in valid_users:\n",
    "        for category in book_categories[row[\"parent_asin\"]]:\n",
    "            users_book_categories_count[row[\"user_id\"]][category] += 1\n",
    "\n",
    "valid_book_categories = set([category for category_counts in users_book_categories_count.values() for category in category_counts.keys()])\n",
    "default_book_categories = {category: 0 for category in valid_book_categories}\n",
    "\n",
    "# this is the dataset we will use for multiple analyses:\n",
    "# sparse dataframe of book categories reviewed per user row.\n",
    "#\n",
    "# option a: 0 v 1 dichotomous indicator of reviewing a book in category C\n",
    "users_book_categories_df = pd.DataFrame([default_book_categories | {category: 1 for category, count in users_book_categories_count[user_id].items()} | {\"user_id\": user_id} for user_id in valid_users])\n",
    "# option b: weighted by # books\n",
    "# users_book_categories_df = pd.DataFrame([default_book_categories | {category: count / np.sum(list(users_book_categories_count[user_id].values())) for category, count in users_book_categories_count[user_id].items()} | {\"user_id\": user_id} for user_id in valid_users])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534f368-8f0e-4382-b9d2-c0ee47d02c23",
   "metadata": {},
   "source": [
    "# Ranking Model Creation\n",
    "\n",
    "Simple SVD recommender over the Amazon book reviews which is used to product `prediction_df`, one real value and one prediction per user in a test group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dfac3-0706-487d-90bb-e96364887008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightweight singular value decomposition-based recommendation engine\n",
    "recommender = surprise.SVD()\n",
    "reader = surprise.Reader(rating_scale=(1, 5))\n",
    "recsys_train_data = surprise.Dataset.load_from_df(recsys_train_df[[\"user_id\", \"parent_asin\", \"rating\"]], reader).build_full_trainset()\n",
    "recommender.fit(recsys_train_data)\n",
    "\n",
    "# the prediction dataframe has a row per prediction and its outcome\n",
    "# because of the nature of the train/test split, there is one row per user in test\n",
    "prediction_df = pd.DataFrame(recommender.test([(row[\"user_id\"], row[\"parent_asin\"], row[\"rating\"]) for _, row in recsys_test_df.iterrows()])).rename(columns={\"uid\": \"user_id\", \"iid\": \"parent_asin\", \"r_ui\": \"rating\", \"est\": \"prediction\"})\n",
    "prediction_df[\"rating\"] = prediction_df[\"rating\"].astype(float)\n",
    "prediction_df[\"delta\"] = prediction_df[\"prediction\"] - prediction_df[\"rating\"]\n",
    "prediction_df[\"delta_zscore\"] = scipy.stats.zscore(prediction_df[\"delta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06a5b7-3ec2-49eb-bc4c-327f3ed7bb8e",
   "metadata": {},
   "source": [
    "## Dataset Finalization and Cleanup\n",
    "\n",
    "`interest_model_df` is the final dataset we will use for all analysis (recommendations, grade level, interests, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef15a09-545c-4fb6-9e12-63b977193a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_books_df = pd.DataFrame(users_books.items(), columns=[\"user_id\", \"books\"])\n",
    "interest_model_df = users_book_categories_df.merge(users_books_df, how=\"inner\", on=\"user_id\").merge(users_textscore_df, how=\"inner\", on=\"user_id\").merge(prediction_df, how=\"inner\", on=\"user_id\")\n",
    "has_category_mask = ((0 < interest_model_df[list(valid_book_categories)].sum(axis=1)) & (interest_model_df[list(valid_book_categories)].sum(axis=1) < np.inf)).to_numpy()\n",
    "interest_model_df = interest_model_df.iloc[has_category_mask]\n",
    "\n",
    "# backup data for easier use in other notebooks\n",
    "with open(\"./data/valid_book_categories.pkl\", \"wb\") as file:\n",
    "    pickle.dump(valid_book_categories, file)\n",
    "interest_model_df.to_pickle(\"./data/interest_model_df.pkl\")\n",
    "\n",
    "# free up memory for clustering\n",
    "del review_data, book_metadata\n",
    "del recommender\n",
    "del recsys_train_df, recsys_test_df, recsys_train_data\n",
    "del users_text\n",
    "del book_titles, book_categories\n",
    "del users_books, users_book_categories_count, default_book_categories\n",
    "del users_book_categories_df, users_books_df, users_textscore_df, prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06209d4d-0cb8-4a7d-9334-1f960ac93267",
   "metadata": {},
   "source": [
    "# Demonstrating Feature Miscalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ef07e-8900-45c6-9f83-817dea1a4349",
   "metadata": {},
   "source": [
    "## Miscalibration due to (Comment Text) Grade Level\n",
    "\n",
    "This cell demonstrations how to proportionally size opportunities according to the miscalibration of our algorithm based on the feature in question. Here, we see that grade-level appears to matter to our model performance.\n",
    "\n",
    "Additional notes of items out of scope of demonstration:\n",
    "\n",
    "* `rating` and `delta` are rather correlated, we would look into this normally as this could bias our results.\n",
    "* A cursory review suggests that the extremes of grade-level are biased by users with poorly formatted or repeatitive text in their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9426a78-f952-417f-ac3f-a30aaa00e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Table:\")\n",
    "display(interest_model_df[[\"rating\", \"delta\", \"delta_zscore\", \"grade_level\", \"fog_index\"]].corr())\n",
    "\n",
    "# this chart renders two calibration analyses, one random and one based on grade level\n",
    "# what we see is that grade-level appears to matter in our predictions!\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 12))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "df = interest_model_df.copy()\n",
    "df[\"grade_level_decile\"] = pd.qcut(np.random.rand(len(df)), 10, labels=False)\n",
    "df = df.groupby(by=[\"grade_level_decile\"]).agg({\"delta\": \"mean\"})\n",
    "\n",
    "axes[0].plot(df[\"delta\"])\n",
    "axes[0].hlines(0, 0, 9, color=\"black\", linestyle=\":\")\n",
    "axes[0].set_title(\"Random Value Calibration Curve: ŷ - y\", fontsize=16)\n",
    "axes[0].set_xlabel(\"Random Value Percentile\", fontsize=12)\n",
    "axes[0].set_xticks(df.index)\n",
    "axes[0].set_xticklabels([f\"\"\"{i*10+10}th\"\"\" for i in range(10)])\n",
    "axes[0].set_ylabel(\"Miscalibration\", fontsize=12)\n",
    "axes[0].set_ylim((-0.12, 0.12))\n",
    "\n",
    "df = interest_model_df.copy()\n",
    "df[\"grade_level_decile\"] = pd.qcut(df[\"grade_level\"], 10, labels=False)\n",
    "df = df.groupby(by=[\"grade_level_decile\"]).agg({\"delta\": \"mean\"})\n",
    "\n",
    "axes[1].plot(df[\"delta\"])\n",
    "axes[1].hlines(0, 0, 9, color=\"black\", linestyle=\":\")\n",
    "axes[1].set_title(\"Grade-Level Calibration Curve: ŷ - y\", fontsize=16)\n",
    "axes[1].set_xlabel(\"Grade-Level Percentile\", fontsize=12)\n",
    "axes[1].set_xticks(df.index)\n",
    "axes[1].set_xticklabels([f\"\"\"{i*10+10}th\"\"\" for i in range(10)])\n",
    "axes[1].set_ylabel(\"Miscalibration\", fontsize=12)\n",
    "axes[1].set_ylim((-0.12, 0.12))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ee412-ffcd-4c5e-86b3-b73fa3870365",
   "metadata": {},
   "source": [
    "# Algorithm Error Model\n",
    "\n",
    "Here we train our model of book recommender error, ŷ - y. Note that this model is rather odd: we have deliberately selected a situation where the data are:\n",
    "1. Of high dimensionality (~1,100 interests)\n",
    "1. Sparse\n",
    "1. Only weakly related to our outcome variable\n",
    "\n",
    "#3 is the primary challenge and occurs for two reasons:\n",
    "1. The methodology focuses on finding indicators of error. If these were strong predictors, we probably would have included them in the recommendation algorithm already. Accordingly, we often are looking at low performing models.\n",
    "1. This is a toy example with particularly weak indicators\n",
    "\n",
    "Ultimately, this is a very messy area to be modeling but the methodology requires that we handle it well. Some of the modeling choices may reflect this atypical scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce4ba8-97a4-499a-bbdf-47ef857d909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# same samples every time\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "sample_mask = pd.qcut(np.random.rand(len(interest_model_df)), 7, labels=False)\n",
    "interest_model_train_df = interest_model_df.iloc[sample_mask >= 2]\n",
    "interest_model_validation_df = interest_model_df.iloc[sample_mask == 0]\n",
    "interest_model_test_df = interest_model_df.iloc[sample_mask == 1]\n",
    "\n",
    "# CUDA test and train datasets\n",
    "X_train = torch.tensor(interest_model_train_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(interest_model_train_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "X_validation = torch.tensor(interest_model_validation_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_validation = torch.tensor(interest_model_validation_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(interest_model_test_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(interest_model_test_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# building a neural network to predict algorithm error.\n",
    "hidden_layer_width = 12\n",
    "interest_model = nn.Sequential(\n",
    "                     nn.Linear(in_features=X_train.shape[1], out_features=hidden_layer_width),\n",
    "                     nn.BatchNorm1d(hidden_layer_width),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(p=0.3),\n",
    "                     nn.Linear(in_features=hidden_layer_width, out_features=hidden_layer_width),\n",
    "                     nn.BatchNorm1d(hidden_layer_width),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(p=0.1),\n",
    "                     nn.Linear(hidden_layer_width, 1)\n",
    "                 ).to(device=device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(interest_model.parameters(), lr=0.00001, weight_decay=0.0075)\n",
    "\n",
    "n_epochs = 150\n",
    "batch_size = 12\n",
    "# store (and plot) the MSE by epoch for train and test\n",
    "mse_history_train, mse_history_validation = [], []\n",
    "# plot the change in MSE over time\n",
    "mse_plot = MSEPlot()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    interest_model.train()\n",
    "    indices = torch.randperm(len(X_train))\n",
    "    batches = len(X_train) // batch_size\n",
    "    for batch in range(batches):\n",
    "        # random batch.  todo: use a dataloader here\n",
    "        slice_ = indices[batch*batch_size:batch*batch_size+batch_size]\n",
    "        # occassional updates about batch progress\n",
    "        if batch % 1000 == 0:\n",
    "            mse_plot.update(mse_history_train, mse_history_validation, epoch=epoch, batch=batch, max_batches=batches)\n",
    "        \n",
    "        X_batch = X_train[slice_]\n",
    "        y_batch = y_train[slice_]\n",
    "        y_pred = interest_model(X_batch).squeeze()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate MSE per epoch\n",
    "    interest_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_pred = interest_model(X_train).squeeze()\n",
    "        train_mse = float(loss_fn(y_pred, y_train))\n",
    "        mse_history_train.append(train_mse)\n",
    "        y_pred = interest_model(X_validation).squeeze()\n",
    "        validation_mse = float(loss_fn(y_pred, y_validation))\n",
    "        mse_history_validation.append(validation_mse)\n",
    "\n",
    "    # update graph\n",
    "    mse_plot.update(mse_history_train, mse_history_validation, epoch=epoch, batch=batch, max_batches=batches)\n",
    "\n",
    "# ensure graph renders above additional information\n",
    "mse_plot.flush()\n",
    "\n",
    "# prediction datasets\n",
    "interest_model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_train_pred = interest_model(X_train).squeeze()\n",
    "    y_validation_pred = interest_model(X_validation).squeeze()\n",
    "    y_test_pred = interest_model(X_test).squeeze()\n",
    "\n",
    "\n",
    "# we expect the overall correlation to be weak- after all, if we can predict ŷ - y easily, then the model\n",
    "# is missing something super obvious in the data.\n",
    "print(\"Overall y~y_pred correlation\")\n",
    "print(\"Train:\")\n",
    "print(scipy.stats.spearmanr(y_train.cpu().numpy(), y_train_pred.cpu().numpy()))\n",
    "print(\"Validation:\")\n",
    "print(scipy.stats.spearmanr(y_validation.cpu().numpy(), y_validation_pred.cpu().numpy()))\n",
    "\n",
    "# render model architecture\n",
    "interest_model_graph = torchview.draw_graph(interest_model, input_size=(batch_size, X_validation.shape[1]))\n",
    "display(interest_model_graph.visual_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37266f16-8b00-4be6-b84b-041de872df0f",
   "metadata": {},
   "source": [
    "# Calculating SHAP\n",
    "\n",
    "We use the DeepExplainer to generate SHAP values as it is much faster than the KernelExplainer on torch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed102a8d-20d6-453d-8400-a40e1d19500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Shapley background data needs to be smaller for our code to run quickly, so we use the kmeans\n",
    "# method to identify representative background data.\n",
    "X_train_summary = shap.kmeans(X_train.cpu().detach().numpy(), int(X_train.shape[1] ** 0.5))\n",
    "\n",
    "# get shapley values\n",
    "interest_model.eval() # disable dropout layers\n",
    "explainer = shap.DeepExplainer(interest_model.to(device=device), torch.tensor(X_train_summary.data, device=device, dtype=torch.float32, requires_grad=False))\n",
    "shap_validation = explainer.shap_values(X_validation).squeeze()\n",
    "shap_test = explainer.shap_values(X_test).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bdffd-2ddf-4610-ae06-a0ebda7b3120",
   "metadata": {},
   "source": [
    "# Clustering SHAP\n",
    "\n",
    "Users with similar SHAP values are presumed to have similar algorithm experiences (e.g. similar book interests with similar biases in recommendations).  If we find greater algorithm error in some of these cohorts, then explaining the reason for cohort error will become a key plan for improving user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e34a00-11af-4542-af06-a55186b0be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# cluster shapley values such that each cluster must be at least 1% of users\n",
    "clusterer_validation = hdbscan.HDBSCAN(min_cluster_size=int(shap_validation.shape[0]*0.01), min_samples=int(shap_validation.shape[0]*0.02), prediction_data=True)\n",
    "clusters_validation = clusterer_validation.fit(shap_validation).labels_\n",
    "clusters_test = hdbscan.approximate_predict(clusterer_validation, shap_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619ac8c-3c16-41e6-9a2b-a4f81306b695",
   "metadata": {},
   "source": [
    "# Analyzing SHAP Clusters\n",
    "\n",
    "Here we find clusters of SHAP values (i.e. book recommendation experience) and inspect their defining characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c4878-90cc-4628-8d33-cd3ce8d36df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def format_cluster_df(interest_df, clusters):\n",
    "    \"\"\"Helper function takes the interest DataFrame and our new clusters.\n",
    "    returns a new summary DataFrame over these clusters.\"\"\"\n",
    "    def clean_group_names(row):\n",
    "        if row[\"cluster_id\"] == -1:\n",
    "            return \"Noise\"\n",
    "        return string.ascii_uppercase[row.name-1]\n",
    "\n",
    "    df = interest_df.copy()\n",
    "    df[\"count\"] = 1\n",
    "    df[\"cluster_id\"] = clusters\n",
    "    df[\"interests\"] = df.apply(lambda row: [category for category in valid_book_categories if row[category] > 0], axis=1)\n",
    "    df = df.groupby(by=[\"cluster_id\"]).agg({\"count\": \"count\",\n",
    "                                            \"delta\": \"mean\",\n",
    "                                            \"delta_zscore\": \"mean\",\n",
    "                                            \"interests\": lambda categories_list: [category for categories in categories_list for category in categories],\n",
    "                                            \"books\": lambda books_list: [book for books in books_list for book in books]\n",
    "                                           }).sort_values(by=[\"count\"], ascending=False).reset_index()\n",
    "    df[\"cluster_name\"] = df.apply(clean_group_names, axis=1)\n",
    "    return df[[\"cluster_id\", \"cluster_name\", \"count\", \"delta\", \"delta_zscore\", \"interests\", \"books\"]]\n",
    "\n",
    "# DataFrame summary of clusters\n",
    "clusters_validation_df = format_cluster_df(interest_model_validation_df, clusters_validation)\n",
    "clusters_test_df = format_cluster_df(interest_model_test_df, clusters_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987217a8-eabb-4a20-a5ae-caa208d6a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation:\")\n",
    "display(clusters_validation_df)\n",
    "\n",
    "print(\"Test:\")\n",
    "display(clusters_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13ea13-927e-4ab0-b2ec-cafcf75186c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def list_clusters_to_tfidf(df, column):\n",
    "    \"\"\"Expects a DataFrame with one row per cluster and a column for book interests\n",
    "    and returns a DataFrame with one row per book, columns for groups, and tfidf scores.\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    tfidf_matrix = tfidf.fit_transform(df[column])\n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out()).T.rename(columns=dict(zip(df.index, df[\"cluster_name\"])))\n",
    "\n",
    "# DataFrames of TD-IDF scores against interest (book category) and book title\n",
    "interest_tfidf_validation_df = list_clusters_to_tfidf(clusters_validation_df, \"interests\")\n",
    "interest_tfidf_test_df = list_clusters_to_tfidf(clusters_test_df, \"interests\")\n",
    "book_tfidf_validation_df = list_clusters_to_tfidf(clusters_validation_df, \"books\")\n",
    "book_tfidf_test_df = list_clusters_to_tfidf(clusters_test_df, \"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac17ccb-3713-48a2-8c03-b73075d09bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation:\")\n",
    "display(interest_tfidf_validation_df.sort_values(by=[\"B\"], ascending=False)[0:10])\n",
    "\n",
    "print(\"Test:\")\n",
    "display(interest_tfidf_test_df.sort_values(by=[\"B\"], ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561055c-f0fa-4c86-a361-b22119ba403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation:\")\n",
    "display(book_tfidf_validation_df.sort_values(by=[\"B\"], ascending=False)[0:10])\n",
    "\n",
    "print(\"Test:\")\n",
    "display(book_tfidf_test_df.sort_values(by=[\"B\"], ascending=False)[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
