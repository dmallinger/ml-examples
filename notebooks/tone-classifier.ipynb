{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6de5d4-425d-4f26-8a61-120957fed5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "MIN_TWEET_TOKENS = 15\n",
    "LLM_SAMPLE_SIZE = 6000\n",
    "GENERATE_RAW_TRAINING_DATA = False\n",
    "GENERATE_LLM_TRAINING_DATA = False\n",
    "TRAIN_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbcc773-4d79-4619-b2c5-09c421a6f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import langdetect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torchview\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from datasets import Dataset\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class MSEPlot:\n",
    "    \"\"\"Ugly code! Dynamically updates plots of MSE so that one can visualize \n",
    "    error in real-time as a model trains.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        plt.ion()\n",
    "        self.fig, self.ax = plt.subplots(1, 1)\n",
    "        self.is_flushed = False\n",
    "\n",
    "    def update(self, train_mse, validation_mse, title=\"\", include_mse=True, epoch=None, batch=None, max_batches=None):\n",
    "        if len(title) > 0:\n",
    "            title = f\"{title}\\n\"\n",
    "\n",
    "        if epoch is not None and batch is not None:\n",
    "            if max_batches is not None:\n",
    "                title = f\"{title}\\nEpoch: {epoch}, {batch/max_batches*100:.0f}%\"\n",
    "            else:\n",
    "                title = f\"{title}\\nEpoch: {epoch}, Batch: {batch}\"\n",
    "        elif epoch is not None:\n",
    "            title = f\"{title}\\nEpoch: {epoch}\"\n",
    "\n",
    "        if include_mse and len(train_mse) > 0:\n",
    "            title = f\"{title}\\nTrain: {train_mse[-1]:.3f}, Validation: {validation_mse[-1]:.3f}\"\n",
    "        \n",
    "        self.ax.clear()\n",
    "        self.ax.plot(train_mse)\n",
    "        self.ax.plot(validation_mse)\n",
    "        self.ax.legend([\"Training\", \"Validation\"], loc=\"upper right\")\n",
    "        if len(title) > 0:\n",
    "            self.ax.set_title(title, fontsize=10)\n",
    "        display(self.fig)\n",
    "        clear_output(wait=True)\n",
    "        self.is_flushed = False\n",
    "\n",
    "    def flush(self):\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.is_flushed = True\n",
    "    \n",
    "    def __del__(self):\n",
    "        if not self.is_flushed:\n",
    "            self.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cfd411-f521-4258-aa07-5c3338bb5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "device = \"cuda\"\n",
    "llm = OllamaLLM(model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ad70c0-2525-40f3-a426-517b8d8376d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_RAW_TRAINING_DATA:\n",
    "    records = []\n",
    "    \n",
    "    # dataset: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "    with open(\"/home/mallinger/Code/data/tone-classifier/training.1600000.processed.noemoticon.csv\", mode=\"r\") as fh:\n",
    "        csv_reader = csv.reader(fh)\n",
    "        while True:\n",
    "            try:\n",
    "                row = next(csv_reader)\n",
    "                text = row[5]\n",
    "                if langdetect.detect(text) == 'en':\n",
    "                    records += [(\"sentiment140\", text)]\n",
    "            except (langdetect.LangDetectException, UnicodeDecodeError, IndexError) as e:\n",
    "                pass\n",
    "            except StopIteration:\n",
    "                break\n",
    "    \n",
    "    raw_text_df = pd.DataFrame(records, columns=[\"source\", \"text\"])\n",
    "    raw_text_df.to_pickle(\"/home/mallinger/Code/data/tone-classifier/raw_text_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5475a196-f4ec-4390-b197-3dd6880d2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TONES = {\"formal\": (\"informal and friendly, characterized by bubbly and outgoing language\", \"formal and distanced, characterized by flat and unemotional language\"),\n",
    "         \"optimistic\": (\"pessimistic, characterized by a negative outlook\", \"optimistic, characterized by positive, forward-looking language\"),\n",
    "         \"apologetic\": (\"defensive, characterized by denial and pushback\", \"apologetic, characterized by regret and a focus on restitution\"),\n",
    "         \"compassionate\": (\"insensitive, characterized by a disregard for the feelings of others\", \"warm, gentle, and empathetic, characterized by a deep concern for another's well-being or struggles\"),\n",
    "        }\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", \n",
    "                                              \"\"\"You are a helpful chatbot that is trained to rewrite text in different tones.\n",
    "                                              For example, you can rewrite a text message to be more happy and cheerful.\n",
    "\n",
    "                                              Rewrite the text submitted by the user to be very {description}.  \n",
    "                                              Do not print any other text or characters.\n",
    "                                              \"\"\"),\n",
    "                                             (\"human\", \n",
    "                                              \"{text}\"\n",
    "                                             )])\n",
    "\n",
    "def get_tone_examples(text):\n",
    "    records = []\n",
    "    for tone, (negative_description, positive_description) in TONES.items():\n",
    "        negative_example_text = llm.invoke(template.format_messages(description=negative_description, text=text), keep_alive=0)\n",
    "        positive_example_text = llm.invoke(template.format_messages(description=positive_description, text=text), keep_alive=0)\n",
    "        records += [(tone, text, positive_example_text, negative_example_text)]\n",
    "    return records\n",
    "\n",
    "\n",
    "if GENERATE_LLM_TRAINING_DATA:\n",
    "    records = []\n",
    "    raw_text_df = pd.read_pickle(\"/home/mallinger/Code/data/tone-classifier/raw_text_df\")\n",
    "    corporus = random.sample([text for text in raw_text_df[\"text\"] if len(re.split(r\"\\s+\", text)) >= MIN_TWEET_TOKENS], LLM_SAMPLE_SIZE)\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        futures = [executor.submit(get_tone_examples, text) for text in corporus]\n",
    "        for future in tqdm(as_completed(futures), total=len(corporus)):\n",
    "            records += [future.result()]\n",
    "        records = sum(records, [])  # flatten\n",
    "    text_df = pd.DataFrame(records, columns=[\"topic\", \"original\", \"positive\", \"negative\"])\n",
    "    text_df.to_pickle(\"/home/mallinger/Code/data/tone-classifier/text_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59cc588-e3cc-4957-adba-4e1b768b1eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mallinger/venv/ml/lib/python3.12/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b810b5-24f6-40d5-a4e0-b3def947ce10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m text_df = pd.read_pickle(\u001b[33m\"\u001b[39m\u001b[33m/home/mallinger/Code/data/tone-classifier/text_df\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m formal_df = get_model_df(text_df, \u001b[33m\"\u001b[39m\u001b[33mformal\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m formal_model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m sample_mask = pd.qcut(np.random.rand(\u001b[38;5;28mlen\u001b[39m(formal_df)), \u001b[32m10\u001b[39m, labels=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     31\u001b[39m X_train = formal_df.iloc[sample_mask >= \u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(hidden_layer_width, bert_model, device)\u001b[39m\n\u001b[32m     14\u001b[39m bert_model = BertModel.from_pretrained(bert_model)\n\u001b[32m     15\u001b[39m bert_final_layer_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(bert_model.parameters())[-\u001b[32m1\u001b[39m])\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbert_final_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_layer_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBatchNorm1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_layer_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_layer_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/ml/lib/python3.12/site-packages/torch/cuda/__init__.py:412\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    411\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    416\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "def get_model_df(text_df, tone, bert_model=\"bert-base-cased\"):\n",
    "    df = text_df[text_df[\"topic\"] == tone]\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "    positives = [tokenizer(text, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\") for text in df[\"positive\"]]\n",
    "    negatives = [tokenizer(text, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\") for text in df[\"negative\"]]\n",
    "\n",
    "    return pd.concat([pd.DataFrame({\"score\": np.ones(len(df)), \"text\": df[\"positive\"], \"bert_input\": positives}),\n",
    "                      pd.DataFrame({\"score\": np.zeros(len(df)), \"text\": df[\"negative\"], \"bert_input\": negatives})\n",
    "                     ]).reset_index()\n",
    "\n",
    "\n",
    "def get_model(hidden_layer_width, bert_model=\"bert-base-cased\", device=\"cpu\"):\n",
    "    bert_model = BertModel.from_pretrained(bert_model)\n",
    "    bert_final_layer_size = len(list(bert_model.parameters())[-1])\n",
    "    return nn.Sequential(\n",
    "                         bert_model,\n",
    "                         nn.Linear(in_features=bert_final_layer_size, out_features=hidden_layer_width),\n",
    "                         nn.BatchNorm1d(hidden_layer_width),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(hidden_layer_width, 1)\n",
    "                     ).to(device=device)\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    text_df = pd.read_pickle(\"/home/mallinger/Code/data/tone-classifier/text_df\")\n",
    "    formal_df = get_model_df(text_df, \"formal\")\n",
    "    formal_model = get_model(128, device=device)\n",
    "    \n",
    "    sample_mask = pd.qcut(np.random.rand(len(formal_df)), 10, labels=False)\n",
    "    X_train = formal_df.iloc[sample_mask >= 2, \"text\"]\n",
    "    X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d206ed49-0503-4565-a4b5-b940f1573a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@jennheartsdavid: It is acknowledged that Davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is being suggested that Joan Rivers conside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Notation of Appreciation for @gloriavelez's Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is proposed that arrangements be made for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@skullnik, a greeting has been extended to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>omg i just ordered the cutest birthday dress E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>OMG exam week is FINALLY over!!! I'm soooo don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Haha omg yaaas Razzle u crack me up!!! dont be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Don't stress, we dodged a bullet this time! On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Hey @empiremagazine!!! No worries at all if yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                                               text\n",
       "0        1.0  @jennheartsdavid: It is acknowledged that Davi...\n",
       "4        1.0  It is being suggested that Joan Rivers conside...\n",
       "8        1.0  Notation of Appreciation for @gloriavelez's Pr...\n",
       "12       1.0  It is proposed that arrangements be made for a...\n",
       "16       1.0  @skullnik, a greeting has been extended to you...\n",
       "...      ...                                                ...\n",
       "10996    0.0  omg i just ordered the cutest birthday dress E...\n",
       "11000    0.0  OMG exam week is FINALLY over!!! I'm soooo don...\n",
       "11004    0.0  Haha omg yaaas Razzle u crack me up!!! dont be...\n",
       "11008    0.0  Don't stress, we dodged a bullet this time! On...\n",
       "11012    0.0  Hey @empiremagazine!!! No worries at all if yo...\n",
       "\n",
       "[5508 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "formal_df = get_model_df(text_df, \"formal\")\n",
    "\n",
    "sample_mask = pd.qcut(np.random.rand(len(interest_model_df)), 7, labels=False)\n",
    "interest_model_train_df = interest_model_df.iloc[sample_mask >= 2]\n",
    "interest_model_validation_df = interest_model_df.iloc[sample_mask == 0]\n",
    "interest_model_test_df = interest_model_df.iloc[sample_mask == 1]\n",
    "\n",
    "# CUDA test and train datasets\n",
    "X_train = torch.tensor(interest_model_train_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(interest_model_train_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "X_validation = torch.tensor(interest_model_validation_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_validation = torch.tensor(interest_model_validation_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(interest_model_test_df[list(valid_book_categories)].to_numpy(), device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(interest_model_test_df[\"delta_zscore\"].to_numpy(), device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# building a neural network to predict algorithm error.\n",
    "hidden_layer_width = 12\n",
    "interest_model = nn.Sequential(\n",
    "                     nn.Linear(in_features=X_train.shape[1], out_features=hidden_layer_width),\n",
    "                     nn.BatchNorm1d(hidden_layer_width),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(p=0.3),\n",
    "                     nn.Linear(in_features=hidden_layer_width, out_features=hidden_layer_width),\n",
    "                     nn.BatchNorm1d(hidden_layer_width),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(p=0.1),\n",
    "                     nn.Linear(hidden_layer_width, 1)\n",
    "                 ).to(device=device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(interest_model.parameters(), lr=0.00001, weight_decay=0.0075)\n",
    "\n",
    "n_epochs = 150\n",
    "batch_size = 12\n",
    "# store (and plot) the MSE by epoch for train and test\n",
    "mse_history_train, mse_history_validation = [], []\n",
    "# plot the change in MSE over time\n",
    "mse_plot = MSEPlot()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    interest_model.train()\n",
    "    indices = torch.randperm(len(X_train))\n",
    "    batches = len(X_train) // batch_size\n",
    "    for batch in range(batches):\n",
    "        # random batch.  todo: use a dataloader here\n",
    "        slice_ = indices[batch*batch_size:batch*batch_size+batch_size]\n",
    "        # occassional updates about batch progress\n",
    "        if batch % 2500 == 0:\n",
    "            mse_plot.update(mse_history_train, mse_history_validation, epoch=epoch, batch=batch, max_batches=batches)\n",
    "        \n",
    "        X_batch = X_train[slice_]\n",
    "        y_batch = y_train[slice_]\n",
    "        y_pred = interest_model(X_batch).squeeze()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate MSE per epoch\n",
    "    interest_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_pred = interest_model(X_train).squeeze()\n",
    "        train_mse = float(loss_fn(y_pred, y_train))\n",
    "        mse_history_train.append(train_mse)\n",
    "        y_pred = interest_model(X_validation).squeeze()\n",
    "        validation_mse = float(loss_fn(y_pred, y_validation))\n",
    "        mse_history_validation.append(validation_mse)\n",
    "\n",
    "    # update graph\n",
    "    mse_plot.update(mse_history_train, mse_history_validation, epoch=epoch, batch=batch, max_batches=batches)\n",
    "\n",
    "# ensure graph renders above additional information\n",
    "mse_plot.flush()\n",
    "\n",
    "# prediction datasets\n",
    "interest_model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_train_pred = interest_model(X_train).squeeze()\n",
    "    y_validation_pred = interest_model(X_validation).squeeze()\n",
    "    y_test_pred = interest_model(X_test).squeeze()\n",
    "\n",
    "\n",
    "# we expect the overall correlation to be weak- after all, if we can predict ŷ - y easily, then the model\n",
    "# is missing something super obvious in the data.\n",
    "print(\"Overall y~y_pred correlation\")\n",
    "print(\"Train:\")\n",
    "print(scipy.stats.spearmanr(y_train.cpu().numpy(), y_train_pred.cpu().numpy()))\n",
    "print(\"Validation:\")\n",
    "print(scipy.stats.spearmanr(y_validation.cpu().numpy(), y_validation_pred.cpu().numpy()))\n",
    "\n",
    "# render model architecture\n",
    "interest_model_graph = torchview.draw_graph(interest_model, input_size=(batch_size, X_validation.shape[1]))\n",
    "display(interest_model_graph.visual_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b7278d-4036-453f-a4a4-903aedc007da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic                                                                                                                                                                             formal\n",
       "original                                        @jennheartsdavid hahaha. i know right!! i was like, omgsh david. he makes me laugh lol.  but yeah. he always has great song suggestions!\n",
       "positive    @jennheartsdavid: It is acknowledged that David possesses a certain capacity for inducing laughter. Additionally, his song recommendations are consistently of high quality.\n",
       "negative                                                            omigosh david is the MAN!!! ikr?! he's literally the best thing since sliced bread lol his music taste is on POINT!!\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
